{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dd4db4-0c53-46a7-bb4e-45e697180331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5516360-db01-4f22-a1d0-decdd6d7350b",
   "metadata": {},
   "source": [
    "# Python basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512a8e3-9219-495e-98c9-49dec44a659d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## numpy.repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a6ff6e55-f224-4d56-b3cf-3799c88303d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m= [[[[1.61 0.68 0.06]\n",
      "   [1.53 0.2  0.54]]]], (1, 1, 2, 3)\n",
      "m2= [[[[1.61 0.68 0.06]\n",
      "   [1.53 0.2  0.54]]\n",
      "\n",
      "  [[1.61 0.68 0.06]\n",
      "   [1.53 0.2  0.54]]\n",
      "\n",
      "  [[1.61 0.68 0.06]\n",
      "   [1.53 0.2  0.54]]]\n",
      "\n",
      "\n",
      " [[[1.61 0.68 0.06]\n",
      "   [1.53 0.2  0.54]]\n",
      "\n",
      "  [[1.61 0.68 0.06]\n",
      "   [1.53 0.2  0.54]]\n",
      "\n",
      "  [[1.61 0.68 0.06]\n",
      "   [1.53 0.2  0.54]]]], (2, 3, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "m= np.round(np.random.uniform(0,2,size=(1,1,2,3)), 2)\n",
    "m2= np.repeat(np.repeat(m,3,axis=1),2,axis=0)\n",
    "print(f'm= {m}, {m.shape}')\n",
    "print(f'm2= {m2}, {m2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2afec-007d-4bb2-b5e4-25726c4bf03c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d800198f-3997-4279-b96e-f94c288ad053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A= [[0.39 0.21 0.17 1.15]\n",
      " [1.01 0.96 0.93 1.3 ]\n",
      " [0.53 1.79 1.8  1.5 ]]\n",
      "B= [[-0.74  0.58 -0.18]\n",
      " [ 0.32 -0.65 -0.72]\n",
      " [-0.5  -0.86  0.09]\n",
      " [ 0.87  0.58  0.83]]\n",
      "A@B= [[ 0.6941  0.6105  0.7484]\n",
      " [ 0.2258 -0.084   0.2897]\n",
      " [ 0.5856 -1.5341  0.0228]]\n",
      "ik,kj->ij= [[ 0.6941  0.6105  0.7484]\n",
      " [ 0.2258 -0.084   0.2897]\n",
      " [ 0.5856 -1.5341  0.0228]]\n",
      "A*(B.T)= [[-0.2886  0.0672 -0.085   1.0005]\n",
      " [ 0.5858 -0.624  -0.7998  0.754 ]\n",
      " [-0.0954 -1.2888  0.162   1.245 ]]\n",
      "ij,ji->ij= [[-0.2886  0.0672 -0.085   1.0005]\n",
      " [ 0.5858 -0.624  -0.7998  0.754 ]\n",
      " [-0.0954 -1.2888  0.162   1.245 ]]\n"
     ]
    }
   ],
   "source": [
    "A= np.round(np.random.uniform(0,2,size=(3,4)), 2)\n",
    "B= np.round(np.random.uniform(-1,1,size=(4,3)), 2)\n",
    "print(f'A= {A}')\n",
    "print(f'B= {B}')\n",
    "print(f'A@B= {A@B}')\n",
    "print(f\"ik,kj->ij= {np.einsum('ik,kj->ij',A,B)}\")\n",
    "print(f'A*(B.T)= {A*(B.T)}')\n",
    "print(f\"ij,ji->ij= {np.einsum('ij,ji->ij',A,B)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839e841-92b1-4af3-98aa-02627e667fe3",
   "metadata": {},
   "source": [
    "## Container class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd17d15-71e0-4525-8fb3-1b02d9541523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local variable holder.\n",
    "class TLocalVar:\n",
    "  def __init__(self, l):\n",
    "    self.__dict__= l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd1d5477-0261-4e63-9bcc-bd7f4b6d77b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': 10, 'a': 100}\n",
      "<__main__.TLocalVar object at 0x7fe432f197b8>\n",
      "100 10\n",
      "10000 1000\n",
      "100 10\n",
      "100 10\n"
     ]
    }
   ],
   "source": [
    "def test_f(a):\n",
    "  b= 10\n",
    "  print(locals())\n",
    "  l= TLocalVar(locals())\n",
    "  print(l)\n",
    "  # print(l['a'])\n",
    "  print(l.a,l.b)\n",
    "  l.a*= 100\n",
    "  l.b*= 100\n",
    "  print(l.a,l.b)\n",
    "  print(a,b)\n",
    "  locals()['a']*= 100\n",
    "  print(a,b)\n",
    "test_f(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f4cb81-ee08-4395-ad5b-1bcaf5ad7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Container class to share variables.\n",
    "class TContainer(object):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def __del__(self):\n",
    "    pass\n",
    "  def __str__(self):\n",
    "    return str(self.__dict__)\n",
    "  def __repr__(self):\n",
    "    return str(self.__dict__)\n",
    "  def __iter__(self):\n",
    "    return self.__dict__.itervalues()\n",
    "  def items(self):\n",
    "    return self.__dict__.items()\n",
    "  def iteritems(self):\n",
    "    return self.__dict__.iteritems()\n",
    "  def keys(self):\n",
    "    return self.__dict__.keys()\n",
    "  def values(self):\n",
    "    return self.__dict__.values()\n",
    "  def __getitem__(self,key):\n",
    "    return self.__dict__[key]\n",
    "  def __setitem__(self,key,value):\n",
    "    self.__dict__[key]= value\n",
    "  def __delitem__(self,key):\n",
    "    del self.__dict__[key]\n",
    "  def __contains__(self,key):\n",
    "    return key in self.__dict__\n",
    "  def Cleanup(self):\n",
    "    keys= self.__dict__.keys()\n",
    "    for k in keys:\n",
    "      self.__dict__[k]= None\n",
    "      del self.__dict__[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0676f7cd-3474-4529-a64b-1265cdc7d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "l= TContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76635cb3-5d93-45b1-8fde-b4f225ec8aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for l.i in range(3):\n",
    "  print(l.i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe63e554-3174-4541-a70a-2eb06e562d5e",
   "metadata": {},
   "source": [
    "## Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89518ed4-2d97-42cb-95fd-2cfac6be933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc.NumClasses()= 12\n",
      "X= [-3.0, -2.2222222222222223, -1.4444444444444444, -0.6666666666666665, 0.11111111111111116, 0.8888888888888888, 1.666666666666667, 2.4444444444444446, 3.2222222222222223, 4.0]\n",
      "disc.Encode(x)= [0.0, 0.0, 2.0, 3.0, 5.0, 6.0, 8.0, 9.0, 11.0, 11.0]\n",
      "disc.Encode(np.array(X))= [ 0.  0.  2.  3.  5.  6.  8.  9. 11. 11.]\n",
      "disc.Encode(torch.tensor(X).float())= tensor([ 0,  0,  2,  3,  5,  6,  8,  9, 11, 11])\n",
      "C= [10  9 -1 11 10  6  4  6  0 -1]\n",
      "disc.Decode(c)= [2.75, 2.25, -2.25, 3.25, 2.75, 0.75, -0.25, 0.75, -2.25, -2.25]\n",
      "disc.Decode(np.array(C))= [ 2.75  2.25 -2.25  3.25  2.75  0.75 -0.25  0.75 -2.25 -2.25]\n",
      "disc.Decode(torch.tensor(C))= tensor([ 2.7500,  2.2500, -2.2500,  3.2500,  2.7500,  0.7500, -0.2500,  0.7500,\n",
      "        -2.2500, -2.2500])\n"
     ]
    }
   ],
   "source": [
    "class TDiscretizer(object):\n",
    "  def __init__(self, xmin, xmax, n_div):\n",
    "    self.xmin, self.xmax, self.n_div= xmin, xmax, n_div\n",
    "    self.dx= (xmax-xmin)/n_div\n",
    "    self.ptcls_max= torch.tensor(self.n_div-1)\n",
    "    self.ptcls_min= torch.tensor(0)\n",
    "  def NumClasses(self):  return self.n_div\n",
    "  def Encode(self, x):  \n",
    "    if isinstance(x,torch.Tensor):\n",
    "      return torch.minimum(self.ptcls_max,torch.maximum(self.ptcls_min,torch.floor((x-self.xmin)/self.dx))).long()\n",
    "    return np.minimum(self.n_div-1,np.maximum(0,np.floor((x-self.xmin)/self.dx)))\n",
    "  def Decode(self, c):\n",
    "    if isinstance(c, torch.Tensor):\n",
    "      return (self.xmin+0.5*self.dx)+self.dx*torch.minimum(self.ptcls_max,torch.maximum(self.ptcls_min,c))\n",
    "    return (self.xmin+0.5*self.dx)+self.dx*np.minimum(self.n_div-1,np.maximum(0,c))\n",
    "disc= TDiscretizer(-2.5, 3.5, 12)\n",
    "print(f'disc.NumClasses()= {disc.NumClasses()}')\n",
    "# np.minimum(3,np.array(range(10))), np.minimum(3,0)\n",
    "# torch.minimum(torch.tensor(3),torch.tensor(range(10))), torch.minimum(3,0)\n",
    "# torch.tensor(range(10))-4\n",
    "X= [x for x in np.linspace(-3.,4.,10)]\n",
    "print(f'X= {X}')\n",
    "print(f'disc.Encode(x)= {[disc.Encode(x) for x in X]}')\n",
    "print(f'disc.Encode(np.array(X))= {disc.Encode(np.array(X))}')\n",
    "print(f'disc.Encode(torch.tensor(X).float())= {disc.Encode(torch.tensor(X).float())}')\n",
    "C= np.random.randint(-1,15, size=(10))\n",
    "print(f'C= {C}')\n",
    "print(f'disc.Decode(c)= {[disc.Decode(c) for c in C]}')\n",
    "print(f'disc.Decode(np.array(C))= {disc.Decode(np.array(C))}')\n",
    "print(f'disc.Decode(torch.tensor(C))= {disc.Decode(torch.tensor(C))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2997f12-cea9-4df4-a22c-e85f0ab69a21",
   "metadata": {},
   "source": [
    "# Torch Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef204c-076b-45bc-bbb5-b2f38dae470f",
   "metadata": {},
   "source": [
    "## shape and view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4e2384-ccf9-4aee-9ae5-0d24c703e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_batch= 1\n",
    "n_batch= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4003e627-2780-4b45-9540-ca1a80a21370",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.tensor(np.zeros((n_batch,3,32,32))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1adcd99c-66bb-457d-b631-8469cbea5189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143ea2e8-2fc6-4de0-a432-07604bbb315f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cefe48b-bcfb-498f-98e2-3df3d8ab8e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3072])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(x.size(0), -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b63fd7e-d3ea-4611-b5b4-934fb1d2395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3072, out_features=10, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=torch.nn.Linear(x[0].numel(), 10)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfbf94ba-e349-4e99-a8e2-203fb46672ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0170, -0.0096, -0.0093, -0.0021,  0.0015,  0.0124,  0.0121,  0.0003,\n",
       "        -0.0003, -0.0002], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(x[0].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a42f4b3-532d-433a-9459-fd3c529657ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0170, -0.0096, -0.0093, -0.0021,  0.0015,  0.0124,  0.0121,  0.0003,\n",
       "         -0.0003, -0.0002],\n",
       "        [ 0.0170, -0.0096, -0.0093, -0.0021,  0.0015,  0.0124,  0.0121,  0.0003,\n",
       "         -0.0003, -0.0002],\n",
       "        [ 0.0170, -0.0096, -0.0093, -0.0021,  0.0015,  0.0124,  0.0121,  0.0003,\n",
       "         -0.0003, -0.0002],\n",
       "        [ 0.0170, -0.0096, -0.0093, -0.0021,  0.0015,  0.0124,  0.0121,  0.0003,\n",
       "         -0.0003, -0.0002]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebb019-610e-4640-b9dc-2c352ae9f0da",
   "metadata": {},
   "source": [
    "## cat (concatenate) and stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e80ca54-f1f4-4296-917f-4c4b20f641b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=tensor([2., 3., 4.])\n",
      "x2=tensor([10., 22., 12.])\n",
      "torch.cat=tensor([ 2.,  3.,  4., 10., 22., 12.])\n",
      "torch.stack=tensor([[ 2.,  3.,  4.],\n",
      "        [10., 22., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x1= torch.from_numpy(np.array([2,3,4])).float()\n",
    "x2= torch.from_numpy(np.array([10,22,12])).float()\n",
    "print(f'x1={x1}')\n",
    "print(f'x2={x2}')\n",
    "print(f'torch.cat={torch.cat((x1,x2),axis=0)}')\n",
    "print(f'torch.stack={torch.stack((x1,x2),axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1324f-4709-4b08-866d-fffb0765b492",
   "metadata": {},
   "source": [
    "## view and flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "377877c4-c347-4b59-9914-115cfdcba967",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch= 64\n",
    "x= torch.tensor(np.zeros((n_batch,128,6,6))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e00c52b-8e48-457a-9669-36392a9a5e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 6, 6])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e8c888bc-8e08-42a5-9350-d85c4400cf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4608])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(x.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c265d07a-5ee8-462c-a44a-e911139a30b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4608])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten= torch.nn.Flatten()\n",
    "flatten(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "42c38211-201f-4dc8-879f-54f42e6bf658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4608])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(x,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf474f6-f5be-4e8b-8ce3-cdc939170357",
   "metadata": {},
   "source": [
    "## unflatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "de07d021-d754-4c29-8132-719fa03abd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 6, 6])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Unflatten(1,(128,6,6))(torch.flatten(x,1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95f24a-6364-49df-9ed2-7099f5acfba1",
   "metadata": {},
   "source": [
    "## Unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4dd77471-dcf8-419b-b58c-96b9bc207039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 10]), torch.Size([1, 3, 10, 10]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.zeros(3,10,10)\n",
    "x.shape, x.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1cfa63-7758-474f-b7da-06748cb2e50d",
   "metadata": {},
   "source": [
    "# Torch Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfe6d8-7201-420d-a1d9-965a971f97b1",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef46634-bb1e-4ab8-89d8-ec0f13dc62b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1=Sequential(\n",
      "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (1): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "l2=Sequential(\n",
      "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (1): None\n",
      "  (2): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "l2b=Sequential(\n",
      "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (1): TNoop()\n",
      "  (2): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "l3=Sequential(\n",
      "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (1): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "x=tensor([1., 1., 1.])\n",
      "l1(x)=tensor([-0.2396,  0.3478, -0.1649, -0.6443], grad_fn=<AddBackward0>)\n",
      "l2b(x)=tensor([ 0.1232, -0.1427, -0.7143,  0.4091], grad_fn=<AddBackward0>)\n",
      "l3(x)=tensor([-0.1548, -0.1635,  0.3904, -0.3709], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from ay_torch import Noop, TNoop\n",
    "# print(isinstance(TNoop(),torch.nn.Module))\n",
    "x= torch.ones(3)\n",
    "l1= torch.nn.Sequential(torch.nn.Linear(3,3),torch.nn.Linear(3,4))\n",
    "l2= torch.nn.Sequential(torch.nn.Linear(3,3),None,torch.nn.Linear(3,4))\n",
    "l2b= torch.nn.Sequential(torch.nn.Linear(3,3),TNoop(),torch.nn.Linear(3,4))\n",
    "l3= torch.nn.Sequential(*filter(lambda x:x is not None, (torch.nn.Linear(3,3),None,torch.nn.Linear(3,4))))\n",
    "print(f'l1={l1}')\n",
    "print(f'l2={l2}')\n",
    "print(f'l2b={l2b}')\n",
    "print(f'l3={l3}')\n",
    "print(f'x={x}')\n",
    "print(f'l1(x)={l1(x)}')\n",
    "# print(f'l2(x)={l2(x)}')  #ERROR: TypeError: 'NoneType' object is not callable\n",
    "print(f'l2b(x)={l2b(x)}')\n",
    "print(f'l3(x)={l3(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1f62ef3-d430-4007-8c16-58f109f898de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l4=Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n",
      "l4(x)=tensor([0.4532, 0.0684, 0.5474], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# l3.append(torch.nn.Linear(4,3))  #AttributeError: 'Sequential' object has no attribute 'append'\n",
    "l4= torch.nn.Sequential(l3, torch.nn.Linear(4,3))\n",
    "print(f'l4={l4}')\n",
    "print(f'l4(x)={l4(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e08707-6d53-470c-b050-c5f7162278e8",
   "metadata": {},
   "source": [
    "## BatchNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca62433d-3c13-4617-8341-d337aa4dc4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([[7.5000, 8.0000, 3.5000, 2.0000, 8.5000],\n",
      "        [9.0000, 5.0000, 4.0000, 6.5000, 1.0000],\n",
      "        [9.5000, 5.5000, 3.0000, 1.5000, 0.5000],\n",
      "        [7.0000, 0.0000, 6.0000, 4.5000, 2.5000]]) (shape=torch.Size([4, 5]))\n",
      "bn(x)= tensor([[-0.7276,  1.1630, -0.5488, -0.8078,  1.6853],\n",
      "        [ 0.7276,  0.1292, -0.1098,  1.4291, -0.6663],\n",
      "        [ 1.2127,  0.3015, -0.9879, -1.0563, -0.8231],\n",
      "        [-1.2127, -1.5937,  1.6465,  0.4350, -0.1960]],\n",
      "       grad_fn=<NativeBatchNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "x= torch.tensor(range(20))/2.0\n",
    "x= x[torch.randperm(x.shape[0])].reshape(-1,5)\n",
    "print(f'x= {x} (shape={x.shape})')\n",
    "bn= torch.nn.BatchNorm1d(5)\n",
    "# bn.bias.data.fill_(1e-3)\n",
    "bn.weight.data.fill_(0.)\n",
    "# bn.bias.data= torch.Tensor([1.,2.,3.,4.,5.])*0.1\n",
    "# bn.weight.data= torch.Tensor([5.,4.,3.,2.,1.])*0.1\n",
    "bn.weight.data.fill_(1.)\n",
    "print(f'bn(x)= {bn(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4329175b-4f29-4d94-8337-112e84820da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.]), tensor([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.weight.data, bn.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f9ce4b5-6e62-4ce2-aeb8-55608e98b81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6301,  1.0072, -0.4753, -0.6996,  1.4595],\n",
       "        [ 0.6301,  0.1119, -0.0951,  1.2377, -0.5770],\n",
       "        [ 1.0502,  0.2611, -0.8555, -0.9148, -0.7128],\n",
       "        [-1.0502, -1.3802,  1.4259,  0.3767, -0.1697]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x,axis=0), torch.var(x,axis=0)\n",
    "(x-torch.mean(x,axis=0))/torch.sqrt(torch.var(x,axis=0)) * bn.weight + bn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ddcda507-9171-4748-8aa8-2e1f308501e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "InstanceNorm1d returns 0-filled tensor to 2D tensor.This is because InstanceNorm1d reshapes inputs to(1, N * C, ...) from (N, C,...) and this makesvariances 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-914f3ccaff3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# inn.weight.data= torch.Tensor([5.,4.,3.,2.,1.])*0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# inn.weight.data.fill_(1.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'inn(x)= {inn(x)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         return F.instance_norm(\n\u001b[1;32m     58\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36m_check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             raise ValueError(\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0;34m'InstanceNorm1d returns 0-filled tensor to 2D tensor.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;34m'This is because InstanceNorm1d reshapes inputs to'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;34m'(1, N * C, ...) from (N, C,...) and this makes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: InstanceNorm1d returns 0-filled tensor to 2D tensor.This is because InstanceNorm1d reshapes inputs to(1, N * C, ...) from (N, C,...) and this makesvariances 0."
     ]
    }
   ],
   "source": [
    "inn= torch.nn.InstanceNorm1d(5, affine=True)\n",
    "# inn.bias.data.fill_(1e-3)\n",
    "# inn.weight.data.fill_(0.)\n",
    "# inn.bias.data= torch.Tensor([1.,2.,3.,4.,5.])*0.1\n",
    "# inn.weight.data= torch.Tensor([5.,4.,3.,2.,1.])*0.1\n",
    "# inn.weight.data.fill_(1.)\n",
    "print(f'inn(x)= {inn(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1452707-6a61-431a-a5df-bfdfda03ff71",
   "metadata": {},
   "source": [
    "## Max module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c57a4225-f72c-4f3f-abc4-96232d36dd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[59.0000, 17.0000, 55.5000, 56.5000, 58.5000],\n",
       "          [24.0000, 39.5000, 50.5000, 62.0000, 72.5000],\n",
       "          [28.0000, 42.5000, 41.0000, 51.0000, 36.5000],\n",
       "          [54.0000, 70.0000, 35.5000, 20.0000, 28.5000],\n",
       "          [26.0000, 16.0000,  6.0000, 70.5000, 49.0000]],\n",
       "\n",
       "         [[19.0000,  0.0000, 23.0000, 48.5000, 14.0000],\n",
       "          [65.5000, 36.0000,  4.0000, 64.0000, 12.0000],\n",
       "          [25.0000, 34.0000, 35.0000,  2.5000, 30.5000],\n",
       "          [61.0000, 54.5000, 25.5000, 60.0000, 69.0000],\n",
       "          [65.0000,  1.5000, 68.0000, 45.0000, 32.0000]],\n",
       "\n",
       "         [[19.5000, 31.0000, 57.5000, 13.0000, 46.0000],\n",
       "          [55.0000, 14.5000, 53.5000,  2.0000, 43.5000],\n",
       "          [49.5000, 67.5000, 46.5000, 15.5000, 61.5000],\n",
       "          [34.5000,  3.0000, 10.5000, 44.0000,  1.0000],\n",
       "          [ 9.0000, 38.5000, 60.5000, 33.5000, 72.0000]]],\n",
       "\n",
       "\n",
       "        [[[33.0000, 73.5000, 41.5000,  4.5000, 18.5000],\n",
       "          [31.5000,  3.5000,  5.0000, 63.0000, 62.5000],\n",
       "          [69.5000, 23.5000, 40.5000, 71.5000, 20.5000],\n",
       "          [16.5000, 18.0000, 47.0000, 48.0000, 22.5000],\n",
       "          [29.0000, 74.0000, 74.5000,  9.5000, 27.0000]],\n",
       "\n",
       "         [[29.5000, 63.5000, 67.0000, 17.5000, 66.0000],\n",
       "          [13.5000, 21.0000, 11.0000, 45.5000, 44.5000],\n",
       "          [27.5000, 58.0000, 42.0000,  7.5000, 24.5000],\n",
       "          [ 6.5000, 37.5000, 56.0000,  8.0000, 47.5000],\n",
       "          [40.0000, 50.0000, 73.0000, 53.0000, 52.5000]],\n",
       "\n",
       "         [[68.5000,  8.5000, 52.0000, 26.5000, 43.0000],\n",
       "          [ 0.5000, 51.5000, 57.0000, 66.5000, 22.0000],\n",
       "          [38.0000, 10.0000, 71.0000, 32.5000, 12.5000],\n",
       "          [ 7.0000, 64.5000, 11.5000, 21.5000, 15.0000],\n",
       "          [30.0000, 59.5000, 39.0000, 37.0000,  5.5000]]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.tensor(range(150))/2.0\n",
    "x= x[torch.randperm(x.shape[0])].reshape(-1,3,5,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1fbf9b93-dd99-4933-bc1c-e556677ea1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5, 5])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x, axis=1)[0].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "15923a8d-9903-4a91-a9a5-91b992e4529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TMax(torch.nn.Module):\n",
    "  def forward(self, x):\n",
    "    return torch.max(x, axis=1)[0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "720a3575-d763-4eee-b0cc-d7d1ffec8078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[59.0000, 31.0000, 57.5000, 56.5000, 58.5000],\n",
       "          [65.5000, 39.5000, 53.5000, 64.0000, 72.5000],\n",
       "          [49.5000, 67.5000, 46.5000, 51.0000, 61.5000],\n",
       "          [61.0000, 70.0000, 35.5000, 60.0000, 69.0000],\n",
       "          [65.0000, 38.5000, 68.0000, 70.5000, 72.0000]]],\n",
       "\n",
       "\n",
       "        [[[68.5000, 73.5000, 67.0000, 26.5000, 66.0000],\n",
       "          [31.5000, 51.5000, 57.0000, 66.5000, 62.5000],\n",
       "          [69.5000, 58.0000, 71.0000, 71.5000, 24.5000],\n",
       "          [16.5000, 64.5000, 56.0000, 48.0000, 47.5000],\n",
       "          [40.0000, 74.0000, 74.5000, 53.0000, 52.5000]]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m= TMax()\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47600a-8c7c-49c8-b949-c9c3cc7f9dc4",
   "metadata": {},
   "source": [
    "## AvgPool2d and Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d13bb2ee-dfd8-4bcf-b704-c6f9848099d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2388, 0.1915, 0.6603, 0.9222, 0.2945, 0.0357, 0.7832, 0.1920],\n",
       "          [0.4177, 0.8654, 0.8985, 0.7739, 0.7018, 0.4917, 0.0838, 0.7546],\n",
       "          [0.2234, 0.6977, 0.2054, 0.9994, 0.8951, 0.7883, 0.3776, 0.6775],\n",
       "          [0.4321, 0.6387, 0.0568, 0.0692, 0.7113, 0.3858, 0.5290, 0.1512],\n",
       "          [0.7873, 0.7903, 0.5976, 0.1046, 0.5114, 0.9380, 0.3291, 0.2403],\n",
       "          [0.2715, 0.7936, 0.8609, 0.6254, 0.4417, 0.2772, 0.4428, 0.8376],\n",
       "          [0.5523, 0.5319, 0.9425, 0.3161, 0.6561, 0.8714, 0.4156, 0.9550],\n",
       "          [0.4729, 0.7059, 0.4009, 0.0883, 0.2010, 0.8251, 0.3593, 0.3616]]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img= torch.from_numpy(np.random.uniform(0,1,size=(1,1,8,8))).float()\n",
    "print(img.shape)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f2bfcd8a-9c5e-4c33-93f3-371bc18f58e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4283, 0.8137, 0.3809, 0.4534],\n",
       "          [0.4980, 0.3327, 0.6951, 0.4338],\n",
       "          [0.6607, 0.5471, 0.5421, 0.4624],\n",
       "          [0.5658, 0.4369, 0.6384, 0.5228]]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2= torch.nn.AvgPool2d(kernel_size=2, stride=None, padding=0, ceil_mode=True)(img)\n",
    "print(img2.shape)\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bb007e21-49a5-4cd6-af3e-496a23b556d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4283, 0.4283, 0.8137, 0.8137, 0.3809, 0.3809, 0.4534, 0.4534],\n",
       "          [0.4283, 0.4283, 0.8137, 0.8137, 0.3809, 0.3809, 0.4534, 0.4534],\n",
       "          [0.4980, 0.4980, 0.3327, 0.3327, 0.6951, 0.6951, 0.4338, 0.4338],\n",
       "          [0.4980, 0.4980, 0.3327, 0.3327, 0.6951, 0.6951, 0.4338, 0.4338],\n",
       "          [0.6607, 0.6607, 0.5471, 0.5471, 0.5421, 0.5421, 0.4624, 0.4624],\n",
       "          [0.6607, 0.6607, 0.5471, 0.5471, 0.5421, 0.5421, 0.4624, 0.4624],\n",
       "          [0.5658, 0.5658, 0.4369, 0.4369, 0.6384, 0.6384, 0.5228, 0.5228],\n",
       "          [0.5658, 0.5658, 0.4369, 0.4369, 0.6384, 0.6384, 0.5228, 0.5228]]]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img3= torch.nn.Upsample(scale_factor=2, mode='nearest', align_corners=None)(img2)\n",
    "print(img3.shape)\n",
    "img3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0872f2e3-dc97-47fc-91b6-716ede87b32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4283, 0.4283, 0.4283, 0.8137, 0.8137, 0.3809, 0.3809, 0.3809,\n",
       "           0.4534, 0.4534],\n",
       "          [0.4283, 0.4283, 0.4283, 0.8137, 0.8137, 0.3809, 0.3809, 0.3809,\n",
       "           0.4534, 0.4534],\n",
       "          [0.4283, 0.4283, 0.4283, 0.8137, 0.8137, 0.3809, 0.3809, 0.3809,\n",
       "           0.4534, 0.4534],\n",
       "          [0.4980, 0.4980, 0.4980, 0.3327, 0.3327, 0.6951, 0.6951, 0.6951,\n",
       "           0.4338, 0.4338],\n",
       "          [0.4980, 0.4980, 0.4980, 0.3327, 0.3327, 0.6951, 0.6951, 0.6951,\n",
       "           0.4338, 0.4338],\n",
       "          [0.6607, 0.6607, 0.6607, 0.5471, 0.5471, 0.5421, 0.5421, 0.5421,\n",
       "           0.4624, 0.4624],\n",
       "          [0.6607, 0.6607, 0.6607, 0.5471, 0.5471, 0.5421, 0.5421, 0.5421,\n",
       "           0.4624, 0.4624],\n",
       "          [0.6607, 0.6607, 0.6607, 0.5471, 0.5471, 0.5421, 0.5421, 0.5421,\n",
       "           0.4624, 0.4624],\n",
       "          [0.5658, 0.5658, 0.5658, 0.4369, 0.4369, 0.6384, 0.6384, 0.6384,\n",
       "           0.5228, 0.5228],\n",
       "          [0.5658, 0.5658, 0.5658, 0.4369, 0.4369, 0.6384, 0.6384, 0.6384,\n",
       "           0.5228, 0.5228]]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img4= torch.nn.Upsample(size=(10,10), mode='nearest', align_corners=None)(img2)\n",
    "print(img4.shape)\n",
    "img4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5c705-9340-4532-a589-cb516dbe9787",
   "metadata": {},
   "source": [
    "## Unfold+bmm and Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e4a9d28-99e4-4c39-9f8a-21e9d6712815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img= torch.from_numpy(np.random.uniform(0,1,size=(1,1,4,4))).float()\n",
    "img= torch.tensor(range(16)).float().reshape(1,1,4,4)\n",
    "print(img.shape)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43145320-5081-4901-822f-28b905356ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: []\n",
      "torch.Size([1, 4, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  4.,  5.,  6.,  8.,  9., 10.],\n",
       "         [ 1.,  2.,  3.,  5.,  6.,  7.,  9., 10., 11.],\n",
       "         [ 4.,  5.,  6.,  8.,  9., 10., 12., 13., 14.],\n",
       "         [ 5.,  6.,  7.,  9., 10., 11., 13., 14., 15.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfold= torch.nn.Unfold(kernel_size=(2,2))\n",
    "print(f'parameters: {[p for p in unfold.parameters()]}')\n",
    "img_u= unfold(img)\n",
    "print(img_u.shape)\n",
    "img_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a793cf43-2e57-48c1-ae51-8630596acb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: tensor([[[[-0.4410, -0.1046],\n",
      "          [ 0.4316,  0.4210]]]])\n",
      "unfold(weight): tensor([[[-0.4410],\n",
      "         [-0.1046],\n",
      "         [ 0.4316],\n",
      "         [ 0.4210]]]), torch.Size([1, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "conv= torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(2,2),bias=False)\n",
    "print(f'weight: {conv.weight.data}')\n",
    "print(f'unfold(weight): {unfold(conv.weight.data)}, {unfold(conv.weight.data).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7e340a9-b460-47cd-9560-57f5f691eeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv(img): tensor([[[[3.7269, 4.0339, 4.3409],\n",
      "          [4.9549, 5.2619, 5.5690],\n",
      "          [6.1830, 6.4900, 6.7970]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[3.7269, 4.0339, 4.3409],\n",
       "          [4.9549, 5.2619, 5.5690],\n",
       "          [6.1830, 6.4900, 6.7970]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'conv(img): {conv(img)}')\n",
    "torch.bmm(img_u.transpose(2,1), unfold(conv.weight.data)).view(-1,1,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52ede4-ee05-45a5-8f19-960b92c2a65d",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "301132ee-7be9-49ad-b9a2-754fca389097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ay_torch import ConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6350e53c-623f-4c02-a5d2-16da3456dde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2839, 0.8808, 0.0139, 0.8287, 0.5301, 0.6634, 0.6912, 0.9814],\n",
       "          [0.1733, 0.3598, 0.4103, 0.4994, 0.0770, 0.8473, 0.9285, 0.0275],\n",
       "          [0.3501, 0.0781, 0.8500, 0.1458, 0.3770, 0.5725, 0.0959, 0.7596],\n",
       "          [0.3627, 0.7821, 0.7763, 0.7251, 0.4945, 0.2048, 0.5201, 0.4572],\n",
       "          [0.2111, 0.1763, 0.3066, 0.7494, 0.9515, 0.7900, 0.2000, 0.5780],\n",
       "          [0.3230, 0.6924, 0.6222, 0.3055, 0.3279, 0.3511, 0.2773, 0.9002],\n",
       "          [0.1326, 0.8908, 0.0514, 0.3623, 0.6731, 0.9318, 0.0854, 0.5532],\n",
       "          [0.4673, 0.3573, 0.9547, 0.6804, 0.6719, 0.8438, 0.7647, 0.1715]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img= torch.from_numpy(np.random.uniform(0,1,size=(1,1,8,8))).float()\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e7b1da3-581f-49d9-94bb-3c5d7567e0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0390, 0.8798, 0.0000, 0.1952, 1.0140, 0.0000, 0.0000, 1.3001],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1778, 1.4182],\n",
      "          [0.4135, 0.0000, 0.0000, 0.1114, 0.0000, 0.0000, 1.1794, 0.3791],\n",
      "          [0.0000, 0.5827, 1.8779, 0.4145, 0.0000, 0.0203, 0.0000, 1.8402],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 1.6081, 1.3924, 0.0000, 0.5100],\n",
      "          [0.0000, 0.0000, 0.7051, 0.4953, 0.0000, 0.1407, 0.0000, 1.0345],\n",
      "          [0.0000, 0.0000, 0.0998, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3873, 1.3679, 0.8526, 0.6056, 0.1805, 1.2865, 1.7175, 0.3159]]]],\n",
      "       grad_fn=<ReluBackward1>)\n",
      "torch.Size([1, 1, 8, 8]) torch.Size([1, 3, 8, 8]) torch.Size([1, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# ConvLayer(1,3,3)(img)\n",
    "# ConvLayer(1,1,3,transpose=True)(img)\n",
    "print(ConvLayer(3,1,3,transpose=True)(ConvLayer(1,3,3)(img)))\n",
    "print(img.shape, ConvLayer(1,3,3)(img).shape, ConvLayer(3,1,3,transpose=True)(ConvLayer(1,3,3)(img)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "544aa322-a9a5-4756-8fa0-d2a83dd1fb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.2139, 0.0000, 0.1058, 0.0000],\n",
      "          [0.3911, 0.0000, 1.5457, 0.0000],\n",
      "          [0.8706, 0.1721, 0.0000, 0.0000],\n",
      "          [1.2724, 0.0000, 0.0813, 0.0000]],\n",
      "\n",
      "         [[0.9741, 0.0000, 1.2964, 0.0000],\n",
      "          [0.6860, 0.2006, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 1.2277, 0.2832],\n",
      "          [0.0000, 0.0000, 1.4465, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.9920, 0.0000, 1.9861],\n",
      "          [0.0000, 0.0000, 0.7416, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.9116, 0.0000, 2.2223]]]], grad_fn=<ReluBackward1>)\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.4904, 0.0000, 0.0000, 0.0000, 2.1680],\n",
      "          [0.2877, 0.6108, 0.1776, 0.6291, 0.0000, 4.0315, 0.9366, 3.0488],\n",
      "          [0.2264, 1.2161, 0.3334, 0.0000, 0.0000, 0.0000, 0.0000, 0.0207],\n",
      "          [0.0000, 1.0006, 0.0000, 0.6195, 0.0000, 0.5516, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5167, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3972, 0.0000, 0.0111, 0.0000, 0.0000, 0.0000, 0.3553],\n",
      "          [0.0000, 0.2710, 0.0000, 0.1401, 0.0000, 0.0000, 0.0000, 0.2127],\n",
      "          [0.2280, 0.1566, 0.0000, 0.1399, 0.0000, 1.0671, 0.0000, 0.3526]]]],\n",
      "       grad_fn=<ReluBackward1>)\n",
      "torch.Size([1, 1, 8, 8]) torch.Size([1, 3, 4, 4]) torch.Size([1, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "print(ConvLayer(1,3,3,stride=2)(img))\n",
    "print(ConvLayer(3,1,3,stride=2,transpose=True)(ConvLayer(1,3,3,stride=2)(img)))\n",
    "print(img.shape, ConvLayer(1,3,3,stride=2)(img).shape, ConvLayer(3,1,3,stride=2,transpose=True)(ConvLayer(1,3,3,stride=2)(img)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9f755-c3be-487b-acd3-f3ddb98b32bf",
   "metadata": {},
   "source": [
    "## ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ff530a5-c164-4f67-b68f-341ae10cb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ay_torch import TResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "12791975-ad44-43be-bd47-cfc5b5549265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TResBlock                                --                        --\n",
       "├─Sequential: 1-1                        [1, 3, 8, 8]              --\n",
       "│    └─Sequential: 2-1                   [1, 3, 8, 8]              --\n",
       "│    │    └─Conv2d: 3-1                  [1, 3, 8, 8]              27\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 3, 8, 8]              6\n",
       "│    │    └─ReLU: 3-3                    [1, 3, 8, 8]              --\n",
       "│    └─Sequential: 2-2                   [1, 3, 8, 8]              --\n",
       "│    │    └─Conv2d: 3-4                  [1, 3, 8, 8]              81\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 3, 8, 8]              6\n",
       "├─Sequential: 1-2                        [1, 3, 8, 8]              --\n",
       "│    └─Sequential: 2-3                   [1, 3, 8, 8]              --\n",
       "│    │    └─Conv2d: 3-6                  [1, 3, 8, 8]              3\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 3, 8, 8]              6\n",
       "├─ReLU: 1-3                              [1, 3, 8, 8]              --\n",
       "==========================================================================================\n",
       "Total params: 129\n",
       "Trainable params: 129\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(img.shape)\n",
    "torchinfo.summary(TResBlock(1,1,3),img.shape)\n",
    "# TResBlock(1,1,3)(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "23f02038-15b6-46a3-863c-6d9ab19adeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TResBlock                                --                        --\n",
       "├─Sequential: 1-1                        [1, 1, 8, 8]              --\n",
       "│    └─Sequential: 2-1                   [1, 1, 8, 8]              --\n",
       "│    │    └─ConvTranspose2d: 3-1         [1, 1, 8, 8]              27\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 1, 8, 8]              2\n",
       "│    │    └─ReLU: 3-3                    [1, 1, 8, 8]              --\n",
       "│    └─Sequential: 2-2                   [1, 1, 8, 8]              --\n",
       "│    │    └─ConvTranspose2d: 3-4         [1, 1, 8, 8]              9\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 1, 8, 8]              2\n",
       "├─Sequential: 1-2                        [1, 1, 8, 8]              --\n",
       "│    └─Sequential: 2-3                   [1, 1, 8, 8]              --\n",
       "│    │    └─ConvTranspose2d: 3-6         [1, 1, 8, 8]              3\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 1, 8, 8]              2\n",
       "├─ReLU: 1-3                              [1, 1, 8, 8]              --\n",
       "==========================================================================================\n",
       "Total params: 45\n",
       "Trainable params: 45\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2= TResBlock(1,1,3)(img)\n",
    "print(img2.shape)\n",
    "torchinfo.summary(TResBlock(1,3,1,transpose=True),img2.shape)\n",
    "# TResBlock(1,3,1)(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c9e2e1d-0f16-45bd-8bf9-b12b085a21af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TResBlock                                --                        --\n",
       "├─Sequential: 1-1                        [1, 3, 4, 4]              --\n",
       "│    └─Sequential: 2-1                   [1, 3, 4, 4]              --\n",
       "│    │    └─Conv2d: 3-1                  [1, 3, 4, 4]              27\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 3, 4, 4]              6\n",
       "│    │    └─ReLU: 3-3                    [1, 3, 4, 4]              --\n",
       "│    └─Sequential: 2-2                   [1, 3, 4, 4]              --\n",
       "│    │    └─Conv2d: 3-4                  [1, 3, 4, 4]              81\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 3, 4, 4]              6\n",
       "├─Sequential: 1-2                        [1, 3, 4, 4]              --\n",
       "│    └─AvgPool2d: 2-3                    [1, 1, 4, 4]              --\n",
       "│    └─Sequential: 2-4                   [1, 3, 4, 4]              --\n",
       "│    │    └─Conv2d: 3-6                  [1, 3, 4, 4]              3\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 3, 4, 4]              6\n",
       "├─ReLU: 1-3                              [1, 3, 4, 4]              --\n",
       "==========================================================================================\n",
       "Total params: 129\n",
       "Trainable params: 129\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(img.shape)\n",
    "torchinfo.summary(TResBlock(1,1,3,stride=2),img.shape)\n",
    "# TResBlock(1,1,3,stride=2)(img)\n",
    "# rb= TResBlock(1,1,3,stride=2)\n",
    "# rb.convpath(img).shape\n",
    "# rb.idpath(img).shape\n",
    "# rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "abf1760a-8788-4e5e-9bb4-4d985faf4a74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TResBlock                                --                        --\n",
       "├─Sequential: 1-1                        [1, 1, 8, 8]              --\n",
       "│    └─Sequential: 2-1                   [1, 1, 8, 8]              --\n",
       "│    │    └─ConvTranspose2d: 3-1         [1, 1, 8, 8]              27\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 1, 8, 8]              2\n",
       "│    │    └─ReLU: 3-3                    [1, 1, 8, 8]              --\n",
       "│    └─Sequential: 2-2                   [1, 1, 8, 8]              --\n",
       "│    │    └─ConvTranspose2d: 3-4         [1, 1, 8, 8]              9\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 1, 8, 8]              2\n",
       "├─Sequential: 1-2                        [1, 1, 8, 8]              --\n",
       "│    └─Sequential: 2-3                   [1, 1, 4, 4]              --\n",
       "│    │    └─ConvTranspose2d: 3-6         [1, 1, 4, 4]              3\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 1, 4, 4]              2\n",
       "│    └─Upsample: 2-4                     [1, 1, 8, 8]              --\n",
       "├─ReLU: 1-3                              [1, 1, 8, 8]              --\n",
       "==========================================================================================\n",
       "Total params: 45\n",
       "Trainable params: 45\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2= TResBlock(1,1,3,stride=2)(img)\n",
    "print(img2.shape)\n",
    "torchinfo.summary(TResBlock(1,3,1,stride=2,transpose=True),img2.shape)\n",
    "# TResBlock(1,3,1,stride=2,transpose=True)(img2)\n",
    "# rb= TResBlock(1,3,1,stride=2,transpose=True,upsample_first=True)\n",
    "# print(rb.convpath(img2).shape)\n",
    "# print(rb.idpath(img2).shape)\n",
    "# rb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75a1bd-60a1-4be3-a5f6-e9bf8e23ad23",
   "metadata": {},
   "source": [
    "## ResNet without pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f9d4c58-c1d5-4c09-bc45-77a049b9e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ay_torch import TResBlock, TResNet, TNoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88729523-b2a2-43f2-a24e-37617f1a98fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6807, 0.6319, 0.1488, 0.2314, 0.1645, 0.2275, 0.8301, 0.4215],\n",
       "          [0.8867, 0.2966, 0.9344, 0.6837, 0.9192, 0.1543, 0.9079, 0.7589],\n",
       "          [0.8372, 0.8294, 0.2528, 0.7335, 0.8887, 0.3479, 0.5266, 0.9375],\n",
       "          [0.7423, 0.2302, 0.8697, 0.0619, 0.1730, 0.6311, 0.1207, 0.5635],\n",
       "          [0.4931, 0.7887, 0.2703, 0.6939, 0.4062, 0.9014, 0.7710, 0.3900],\n",
       "          [0.5141, 0.9686, 0.1900, 0.2002, 0.1720, 0.5338, 0.1929, 0.7724],\n",
       "          [0.9152, 0.2015, 0.0836, 0.3036, 0.2042, 0.3879, 0.0426, 0.1872],\n",
       "          [0.9524, 0.5482, 0.6289, 0.8618, 0.2832, 0.5301, 0.9718, 0.6986]]],\n",
       "\n",
       "\n",
       "        [[[0.4640, 0.2365, 0.9946, 0.7539, 0.0234, 0.3142, 0.6843, 0.0659],\n",
       "          [0.1769, 0.6768, 0.4790, 0.5934, 0.4234, 0.1729, 0.3388, 0.0212],\n",
       "          [0.4978, 0.1483, 0.8279, 0.4131, 0.3294, 0.4945, 0.3556, 0.1566],\n",
       "          [0.6589, 0.4510, 0.2039, 0.9783, 0.6589, 0.1906, 0.8111, 0.5160],\n",
       "          [0.6891, 0.9937, 0.1032, 0.4532, 0.0265, 0.9145, 0.3716, 0.3260],\n",
       "          [0.3979, 0.0926, 0.4076, 0.2745, 0.7731, 0.4823, 0.9309, 0.9753],\n",
       "          [0.7954, 0.9139, 0.7152, 0.3821, 0.3809, 0.7550, 0.2252, 0.4030],\n",
       "          [0.0217, 0.5690, 0.4489, 0.2885, 0.0212, 0.0437, 0.5431, 0.0351]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img= torch.from_numpy(np.random.uniform(0,1,size=(2,1,8,8))).float()\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f203bf49-c734-4fc2-9e15-2b061f597c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 8, 8])\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "TResNet                                       --                        --\n",
      "├─Sequential: 1-1                             [2, 32, 8, 8]             --\n",
      "│    └─Conv2d: 2-1                            [2, 32, 8, 8]             288\n",
      "│    └─BatchNorm2d: 2-2                       [2, 32, 8, 8]             64\n",
      "│    └─ReLU: 2-3                              [2, 32, 8, 8]             --\n",
      "├─Sequential: 1-2                             [2, 32, 4, 4]             --\n",
      "│    └─Conv2d: 2-4                            [2, 32, 4, 4]             9,216\n",
      "│    └─BatchNorm2d: 2-5                       [2, 32, 4, 4]             64\n",
      "│    └─ReLU: 2-6                              [2, 32, 4, 4]             --\n",
      "├─Sequential: 1-3                             [2, 64, 2, 2]             --\n",
      "│    └─Conv2d: 2-7                            [2, 64, 2, 2]             18,432\n",
      "│    └─BatchNorm2d: 2-8                       [2, 64, 2, 2]             128\n",
      "│    └─ReLU: 2-9                              [2, 64, 2, 2]             --\n",
      "├─TNoop: 1-4                                  [2, 64, 2, 2]             --\n",
      "├─Sequential: 1-5                             [2, 64, 2, 2]             --\n",
      "│    └─TResBlock: 2-10                        [2, 64, 2, 2]             --\n",
      "│    │    └─Sequential: 3-1                   [2, 64, 2, 2]             73,984\n",
      "│    │    └─Sequential: 3-2                   [2, 64, 2, 2]             --\n",
      "│    │    └─ReLU: 3-3                         [2, 64, 2, 2]             --\n",
      "│    └─TResBlock: 2-11                        [2, 64, 2, 2]             --\n",
      "│    │    └─Sequential: 3-4                   [2, 64, 2, 2]             73,984\n",
      "│    │    └─Sequential: 3-5                   [2, 64, 2, 2]             --\n",
      "│    │    └─ReLU: 3-6                         [2, 64, 2, 2]             --\n",
      "├─Sequential: 1-6                             [2, 128, 1, 1]            --\n",
      "│    └─TResBlock: 2-12                        [2, 128, 2, 2]            --\n",
      "│    │    └─Sequential: 3-7                   [2, 128, 2, 2]            221,696\n",
      "│    │    └─Sequential: 3-8                   [2, 128, 2, 2]            8,448\n",
      "│    │    └─ReLU: 3-9                         [2, 128, 2, 2]            --\n",
      "│    └─TResBlock: 2-13                        [2, 128, 1, 1]            --\n",
      "│    │    └─Sequential: 3-10                  [2, 128, 1, 1]            295,424\n",
      "│    │    └─Sequential: 3-11                  [2, 128, 1, 1]            --\n",
      "│    │    └─ReLU: 3-12                        [2, 128, 1, 1]            --\n",
      "├─Sequential: 1-7                             [2, 256, 1, 1]            --\n",
      "│    └─TResBlock: 2-14                        [2, 256, 1, 1]            --\n",
      "│    │    └─Sequential: 3-13                  [2, 256, 1, 1]            885,760\n",
      "│    │    └─Sequential: 3-14                  [2, 256, 1, 1]            33,280\n",
      "│    │    └─ReLU: 3-15                        [2, 256, 1, 1]            --\n",
      "│    └─TResBlock: 2-15                        [2, 256, 1, 1]            --\n",
      "│    │    └─Sequential: 3-16                  [2, 256, 1, 1]            1,180,672\n",
      "│    │    └─Sequential: 3-17                  [2, 256, 1, 1]            --\n",
      "│    │    └─ReLU: 3-18                        [2, 256, 1, 1]            --\n",
      "├─Sequential: 1-8                             [2, 512, 1, 1]            --\n",
      "│    └─TResBlock: 2-16                        [2, 512, 1, 1]            --\n",
      "│    │    └─Sequential: 3-19                  [2, 512, 1, 1]            3,540,992\n",
      "│    │    └─Sequential: 3-20                  [2, 512, 1, 1]            132,096\n",
      "│    │    └─ReLU: 3-21                        [2, 512, 1, 1]            --\n",
      "│    └─TResBlock: 2-17                        [2, 512, 1, 1]            --\n",
      "│    │    └─Sequential: 3-22                  [2, 512, 1, 1]            4,720,640\n",
      "│    │    └─Sequential: 3-23                  [2, 512, 1, 1]            --\n",
      "│    │    └─ReLU: 3-24                        [2, 512, 1, 1]            --\n",
      "├─AdaptiveAvgPool2d: 1-9                      [2, 512, 1, 1]            --\n",
      "├─Flatten: 1-10                               [2, 512]                  --\n",
      "├─Dropout: 1-11                               [2, 512]                  --\n",
      "├─Linear: 1-12                                [2, 1]                    513\n",
      "===============================================================================================\n",
      "Total params: 11,195,681\n",
      "Trainable params: 11,195,681\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 25.08\n",
      "===============================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.30\n",
      "Params size (MB): 44.78\n",
      "Estimated Total Size (MB): 45.09\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "# TResNet(TResBlock,1,[2,2,2,2],in_channels=1,out_channels=1)(img)\n",
    "print(torchinfo.summary(TResNet(TResBlock,1,[2,2,2,2],in_channels=1,out_channels=1,rnpool=TNoop),img.shape))\n",
    "# print(torchinfo.summary(TResBlock(1,1,3,stride=2,pool=TNoop),img.shape))\n",
    "# TResBlock(1,1,3)(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8dae63-b518-40bf-8f25-65f3e136760b",
   "metadata": {},
   "source": [
    "# Torch functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356eb25-66bd-4dc3-9f08-dd7c5993c92e",
   "metadata": {},
   "source": [
    "## Softmax and clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "026a0a2f-9cf4-4681-b651-a0149c27fe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1590, -0.4883,  0.0481, -0.2388, -0.4310],\n",
       "          [-0.2900,  0.2253, -0.2798,  0.2368,  0.6311],\n",
       "          [-0.4764,  0.6701,  0.8507,  0.3041,  0.8848],\n",
       "          [ 0.0155, -0.5271, -0.7959,  0.2696,  0.9397],\n",
       "          [ 0.2211,  0.3824, -0.3784,  0.1784,  0.5068]]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img= torch.from_numpy(np.random.uniform(-1,1,size=(1,1,5,5)))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "88691578-1bb1-47d1-b25e-9d9fb26fffc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5397, 0.3803, 0.5120, 0.4406, 0.3939],\n",
       "          [0.4280, 0.5561, 0.4305, 0.5589, 0.6527],\n",
       "          [0.3831, 0.6615, 0.7007, 0.5754, 0.7078],\n",
       "          [0.5039, 0.3712, 0.3109, 0.5670, 0.7190],\n",
       "          [0.5551, 0.5945, 0.4065, 0.5445, 0.6240]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0232e813-cbc7-4fe4-8e36-c41f41fa09dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1590, 0.0000, 0.0481, 0.0000, 0.0000],\n",
       "          [0.0000, 0.2253, 0.0000, 0.2368, 0.6311],\n",
       "          [0.0000, 0.6701, 0.8507, 0.3041, 0.8848],\n",
       "          [0.0155, 0.0000, 0.0000, 0.2696, 0.9397],\n",
       "          [0.2211, 0.3824, 0.0000, 0.1784, 0.5068]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(img,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43d7f5-cd4a-437f-ae6b-c883c543837a",
   "metadata": {},
   "source": [
    "# Implementing convolution from scratch\n",
    "Ref. https://stackoverflow.com/questions/43086557/convolve2d-just-by-using-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854b472-c5d9-4d6b-9110-1f19f41c456c",
   "metadata": {},
   "source": [
    "## Convolution (N-batch=1, N-inchannels=1, N-outchannels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f0c105c-1d80-4798-a9d9-f68003f876d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[0.08 0.93 0.67 0.68 0.33 0.15 0.16 0.76 0.28]\n",
      " [0.48 0.57 0.15 0.68 0.49 0.69 0.39 0.99 0.54]\n",
      " [0.8  0.42 0.61 0.07 0.7  0.22 0.45 0.26 0.88]\n",
      " [0.11 0.35 0.65 0.08 0.65 0.52 0.23 0.86 0.9 ]\n",
      " [0.06 0.68 0.63 0.34 0.46 0.96 0.3  0.82 0.78]\n",
      " [0.22 0.88 0.9  0.02 0.26 0.14 0.57 0.45 0.23]]\n",
      "w= [[-0.92  0.33 -0.08]\n",
      " [-0.4   0.85 -0.37]\n",
      " [-0.91  0.31  0.42]]\n"
     ]
    }
   ],
   "source": [
    "img= np.round(np.random.uniform(0,1,size=(6,9)),2)\n",
    "w= np.round(np.random.uniform(-1,1,size=(3,3)),2)\n",
    "print(f'img= {img}')\n",
    "print(f'w= {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a70a41-dba3-4bd4-8d4c-b7a49d7c2a63",
   "metadata": {},
   "source": [
    "### Ground truth: torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0e8cab15-aaf1-4bf7-a87d-622b6b661309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0751, -1.2047, -0.3211, -0.3938, -0.4005, -0.4083,  0.6076],\n",
       "          [-0.1728, -0.2881, -0.6900,  0.3136, -0.8475, -0.4277,  0.0754],\n",
       "          [-0.2124, -0.0885, -1.3013,  0.7135, -0.5055, -0.8416,  0.2162],\n",
       "          [ 0.7338, -0.4896, -1.4604,  0.1203,  0.1224, -0.6654,  0.0060]]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).reshape((1,1,)+img.shape).float()\n",
    "w_torch= torch.from_numpy(w).reshape((1,1,)+w.shape).float()\n",
    "torch.nn.functional.conv2d(img_torch, w_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35edbaa3-0915-43af-af2a-81216ff9af5b",
   "metadata": {},
   "source": [
    "### With numpy stride_tricks.as_strided and einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "85b75b0a-3baf-463e-9988-9ccbf816c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (4, 7, 3, 3)\n",
      "strides= (72, 8, 72, 8)\n",
      "conv1= [[ 0.0751 -1.2047 -0.3211 -0.3938 -0.4005 -0.4083  0.6076]\n",
      " [-0.1728 -0.2881 -0.69    0.3136 -0.8475 -0.4277  0.0754]\n",
      " [-0.2124 -0.0885 -1.3013  0.7135 -0.5055 -0.8416  0.2162]\n",
      " [ 0.7338 -0.4896 -1.4604  0.1203  0.1224 -0.6654  0.006 ]]\n",
      "conv2= [[ 0.0751 -1.2047 -0.3211 -0.3938 -0.4005 -0.4083  0.6076]\n",
      " [-0.1728 -0.2881 -0.69    0.3136 -0.8475 -0.4277  0.0754]\n",
      " [-0.2124 -0.0885 -1.3013  0.7135 -0.5055 -0.8416  0.2162]\n",
      " [ 0.7338 -0.4896 -1.4604  0.1203  0.1224 -0.6654  0.006 ]]\n"
     ]
    }
   ],
   "source": [
    "view_shape= tuple(np.subtract(img.shape, w.shape)+1)+w.shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img.strides+img.strides\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= np.lib.stride_tricks.as_strided(img, view_shape, strides)\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv1= (sub_matrices*w).sum(axis=(2,3))\n",
    "print(f'conv1= {conv1}')\n",
    "conv2= np.einsum('ij,klij->kl', w, sub_matrices)\n",
    "print(f'conv2= {conv2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee5ca9-97ae-4461-b6bb-195f85ea8e3d",
   "metadata": {},
   "source": [
    "### With Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f64ae9a9-fcf1-47e0-90d6-4ed09ffff214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= torch.Size([1, 1, 4, 7, 3, 3])\n",
      "strides= (54, 54, 9, 1, 9, 1)\n",
      "conv1= tensor([[[[ 0.0751, -1.2047, -0.3211, -0.3938, -0.4005, -0.4083,  0.6076],\n",
      "          [-0.1728, -0.2881, -0.6900,  0.3136, -0.8475, -0.4277,  0.0754],\n",
      "          [-0.2124, -0.0885, -1.3013,  0.7135, -0.5055, -0.8416,  0.2162],\n",
      "          [ 0.7338, -0.4896, -1.4604,  0.1203,  0.1224, -0.6654,  0.0060]]]])\n",
      "conv2= tensor([[[[ 0.0751, -1.2047, -0.3211, -0.3938, -0.4005, -0.4083,  0.6076],\n",
      "          [-0.1728, -0.2881, -0.6900,  0.3136, -0.8475, -0.4277,  0.0754],\n",
      "          [-0.2124, -0.0885, -1.3013,  0.7135, -0.5055, -0.8416,  0.2162],\n",
      "          [ 0.7338, -0.4896, -1.4604,  0.1203,  0.1224, -0.6654,  0.0060]]]])\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).reshape((1,1,)+img.shape).float()\n",
    "w_torch= torch.from_numpy(w).reshape((1,1,)+w.shape).float()\n",
    "view_shape= img_torch.shape[:2]+tuple(np.subtract(img_torch.shape[2:],w_torch.shape[2:])+1)+w_torch.shape[2:]\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img_torch.stride()+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv1= (sub_matrices*w_torch).sum(axis=(4,5))\n",
    "print(f'conv1= {conv1}')\n",
    "conv2= torch.einsum('klij,klmnij->klmn', w_torch, sub_matrices)\n",
    "print(f'conv2= {conv2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91640c-10a3-4fcf-b778-9453d5b45b81",
   "metadata": {},
   "source": [
    "## Convolution (N-batch=1, N-inchannels=2, N-outchannels=3)\n",
    "Note: The following code works with n_bch>=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "966c7e07-b8d0-4777-bb6b-3e83a9fe365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[[[0.58 0.38 0.12 0.17 0.97 0.18 0.95 0.95 0.27]\n",
      "   [0.09 0.86 0.63 0.99 0.84 0.24 0.21 0.26 0.21]\n",
      "   [0.89 0.43 0.9  0.75 0.87 0.16 0.41 0.08 0.11]\n",
      "   [0.16 0.81 0.15 0.56 0.35 0.11 0.41 0.9  0.09]\n",
      "   [0.73 0.43 0.98 0.79 0.52 0.54 0.15 0.25 0.19]\n",
      "   [0.99 0.19 0.04 0.77 0.36 0.47 0.14 0.52 0.15]]\n",
      "\n",
      "  [[0.89 0.98 0.89 0.14 0.78 0.14 0.95 0.95 0.73]\n",
      "   [0.12 0.13 0.87 0.72 0.27 0.96 0.91 0.84 0.14]\n",
      "   [0.37 0.56 0.06 0.5  0.27 0.07 0.57 0.86 0.44]\n",
      "   [0.46 0.13 0.26 0.31 0.9  0.86 0.93 0.24 0.58]\n",
      "   [0.84 0.62 0.58 0.91 0.86 0.59 0.16 0.31 0.32]\n",
      "   [0.39 0.44 0.67 0.37 0.96 0.83 0.26 0.91 0.58]]]]\n",
      "w= [[[[-1.   -0.12 -0.24]\n",
      "   [ 0.5   0.92  0.81]\n",
      "   [ 0.47  0.18 -0.04]]\n",
      "\n",
      "  [[-0.11  0.6   0.16]\n",
      "   [-0.19  0.49 -0.37]\n",
      "   [-0.09  0.44  0.28]]]\n",
      "\n",
      "\n",
      " [[[ 0.5  -0.2   0.45]\n",
      "   [-0.49 -0.34  0.54]\n",
      "   [-0.6   0.16 -0.82]]\n",
      "\n",
      "  [[ 0.42 -0.53 -0.91]\n",
      "   [-0.79  0.3   0.42]\n",
      "   [-0.93 -0.9  -0.54]]]\n",
      "\n",
      "\n",
      " [[[-0.69 -0.89  0.13]\n",
      "   [-0.94 -0.65  0.37]\n",
      "   [-0.46  0.03 -0.45]]\n",
      "\n",
      "  [[-0.3  -0.93  0.54]\n",
      "   [ 0.7  -0.01 -0.04]\n",
      "   [ 0.56  0.51  0.46]]]]\n"
     ]
    }
   ],
   "source": [
    "n_bch,in_ch,out_ch,ks= 1,2,3,3\n",
    "img= np.round(np.random.uniform(0,1,size=(n_bch,in_ch,6,9)),2)\n",
    "w= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch,ks,ks)),2)\n",
    "print(f'img= {img}')\n",
    "print(f'w= {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a582c9-c493-4278-84c8-69d9317f3b40",
   "metadata": {},
   "source": [
    "### Ground truth: torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "da614da6-7c0f-4cda-aa75-8d78326aba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_gt= tensor([[[[ 1.7332,  2.4102,  2.5449,  1.8440,  0.4113,  1.2922,  0.8077],\n",
      "          [ 1.9147,  1.4130,  1.9298,  1.3351,  1.1848,  1.2055,  1.1881],\n",
      "          [ 0.7150,  1.1221,  0.9520,  0.8854,  0.1931,  1.9831,  1.2294],\n",
      "          [ 2.1038,  1.2396,  2.1700,  2.3855,  1.6488,  1.1085,  0.7497]],\n",
      "\n",
      "         [[-2.4579, -1.1591, -2.2823, -2.4235, -1.1120, -2.3014, -2.7134],\n",
      "          [-1.6237, -2.0818, -1.0713, -3.0737, -3.6110, -2.8510, -1.7890],\n",
      "          [-2.7452, -2.2581, -2.3141, -2.4440, -1.6706, -2.3933, -2.0621],\n",
      "          [-2.0524, -1.7402, -2.7161, -3.8014, -3.4150, -2.6239, -1.6516]],\n",
      "\n",
      "         [[-2.0643, -2.1252, -0.7881, -2.5149, -1.4129, -0.2671, -1.1814],\n",
      "          [-0.6381, -2.0971, -2.4688, -1.4834, -0.8247, -0.7548, -0.6436],\n",
      "          [-1.5008, -0.9920, -1.2218, -1.5451,  0.3733,  0.1928, -1.1368],\n",
      "          [-0.6915, -0.7645, -0.4683, -0.9015, -0.1823, -0.8607, -0.6381]]]])\n"
     ]
    }
   ],
   "source": [
    "# print(torch.nn.Conv2d(2,3,bias=False,kernel_size=(3,3)).weight.shape)\n",
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "conv_gt= torch.nn.functional.conv2d(img_torch, w_torch)\n",
    "print(f'conv_gt= {conv_gt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36442e45-a63d-47e1-a56b-1ceafc06089c",
   "metadata": {},
   "source": [
    "### With numpy stride_tricks.as_strided and einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4a0a02d8-faeb-4523-af5f-0ca5486ae02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (1, 2, 4, 7, 3, 3)\n",
      "strides= (864, 432, 72, 8, 72, 8)\n",
      "sub_matrices.shape= (1, 2, 4, 7, 3, 3)\n",
      "w.shape= (3, 2, 3, 3)\n",
      "conv_np= [[[[ 1.7332  2.4102  2.5449  1.844   0.4113  1.2922  0.8077]\n",
      "   [ 1.9147  1.413   1.9298  1.3351  1.1848  1.2055  1.1881]\n",
      "   [ 0.715   1.1221  0.952   0.8854  0.1931  1.9831  1.2294]\n",
      "   [ 2.1038  1.2396  2.17    2.3855  1.6488  1.1085  0.7497]]\n",
      "\n",
      "  [[-2.4579 -1.1591 -2.2823 -2.4235 -1.112  -2.3014 -2.7134]\n",
      "   [-1.6237 -2.0818 -1.0713 -3.0737 -3.611  -2.851  -1.789 ]\n",
      "   [-2.7452 -2.2581 -2.3141 -2.444  -1.6706 -2.3933 -2.0621]\n",
      "   [-2.0524 -1.7402 -2.7161 -3.8014 -3.415  -2.6239 -1.6516]]\n",
      "\n",
      "  [[-2.0643 -2.1252 -0.7881 -2.5149 -1.4129 -0.2671 -1.1814]\n",
      "   [-0.6381 -2.0971 -2.4688 -1.4834 -0.8247 -0.7548 -0.6436]\n",
      "   [-1.5008 -0.992  -1.2218 -1.5451  0.3733  0.1928 -1.1368]\n",
      "   [-0.6915 -0.7645 -0.4683 -0.9015 -0.1823 -0.8607 -0.6381]]]], True\n"
     ]
    }
   ],
   "source": [
    "view_shape= img.shape[:2]+tuple(np.subtract(img.shape[2:], w.shape[-2:])+1)+w.shape[-2:]\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img.strides+img.strides[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= np.lib.stride_tricks.as_strided(img, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "print(f'w.shape= {w.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv_np= np.einsum('ijkl,bjmnkl->bimn', w, sub_matrices)\n",
    "print(f'conv_np= {conv_np}, {torch.all(torch.isclose(conv_gt,torch.from_numpy(conv_np).float()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b2d73-1cc6-4cb4-8c2c-a2227d9e3968",
   "metadata": {},
   "source": [
    "### With Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dc1022c7-8e44-487f-86ad-29dac907932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= torch.Size([1, 2, 4, 7, 3, 3])\n",
      "strides= (108, 54, 9, 1, 9, 1)\n",
      "sub_matrices.shape= torch.Size([1, 2, 4, 7, 3, 3])\n",
      "conv_tch= tensor([[[[ 1.7332,  2.4102,  2.5449,  1.8440,  0.4113,  1.2922,  0.8077],\n",
      "          [ 1.9147,  1.4130,  1.9298,  1.3351,  1.1848,  1.2055,  1.1881],\n",
      "          [ 0.7150,  1.1221,  0.9520,  0.8854,  0.1931,  1.9831,  1.2294],\n",
      "          [ 2.1038,  1.2396,  2.1700,  2.3855,  1.6488,  1.1085,  0.7497]],\n",
      "\n",
      "         [[-2.4579, -1.1591, -2.2823, -2.4235, -1.1120, -2.3014, -2.7134],\n",
      "          [-1.6237, -2.0818, -1.0713, -3.0737, -3.6110, -2.8510, -1.7890],\n",
      "          [-2.7452, -2.2581, -2.3141, -2.4440, -1.6706, -2.3933, -2.0621],\n",
      "          [-2.0524, -1.7402, -2.7161, -3.8014, -3.4150, -2.6239, -1.6516]],\n",
      "\n",
      "         [[-2.0643, -2.1252, -0.7881, -2.5149, -1.4129, -0.2671, -1.1814],\n",
      "          [-0.6381, -2.0971, -2.4688, -1.4834, -0.8247, -0.7548, -0.6436],\n",
      "          [-1.5008, -0.9920, -1.2218, -1.5451,  0.3733,  0.1928, -1.1368],\n",
      "          [-0.6915, -0.7645, -0.4683, -0.9015, -0.1823, -0.8607, -0.6381]]]]), True\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "view_shape= img_torch.shape[:2]+tuple(np.subtract(img_torch.shape[2:],w_torch.shape[-2:])+1)+w_torch.shape[-2:]\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img_torch.stride()+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv_tch= torch.einsum('ijkl,bjmnkl->bimn', w_torch, sub_matrices)\n",
    "print(f'conv_tch= {conv_tch}, {torch.all(torch.isclose(conv_gt,conv_tch))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22c7e5-5226-4532-a787-7e914d5a9d0b",
   "metadata": {},
   "source": [
    "## Pixel-variant kernel (N-batch=1, N-inchannels=1, N-outchannels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "293250f8-9dc4-4269-9e0a-b793cca000c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[0.78 0.79 0.32 0.44 0.79 0.26 0.59 0.34 0.38]\n",
      " [0.94 0.62 0.43 0.03 0.82 0.45 0.63 0.64 0.43]\n",
      " [0.67 0.69 0.81 0.63 0.37 0.91 0.35 0.36 0.56]\n",
      " [0.02 0.74 0.88 0.68 0.51 0.15 0.41 0.68 0.29]\n",
      " [0.58 0.9  0.23 0.1  0.26 0.06 0.35 0.13 0.84]\n",
      " [0.57 0.67 0.11 0.6  0.3  0.58 0.75 0.15 0.06]]\n",
      "w= [[[[ 0.12  0.07  0.34]\n",
      "   [ 0.07  0.27  0.67]\n",
      "   [ 0.55 -0.11  0.77]]]]..., (4, 7, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "kernel_shape= (3,3)\n",
    "img= np.round(np.random.uniform(0,1,size=(6,9)),2)\n",
    "out_shape= tuple(np.subtract(img.shape, kernel_shape)+1)\n",
    "w= np.round(np.random.uniform(-1,1,size=out_shape+kernel_shape),2)\n",
    "print(f'img= {img}')\n",
    "print(f'w= {w[0:1,0:1,:,:]}..., {w.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08220d0-472f-4913-b221-ab40f613ebd4",
   "metadata": {},
   "source": [
    "### With numpy stride_tricks.as_strided and einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "41b60cad-aabb-4b29-bc94-03617e28d072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (4, 7, 3, 3)\n",
      "strides= (72, 8, 72, 8)\n",
      "sub_matrices.shape= (4, 7, 3, 3)\n",
      "res1= [[ 1.6953 -2.4214  0.5561  0.3529 -0.6678  0.8172  0.7055]\n",
      " [-1.5766 -1.3174 -1.0877  1.7223  1.1279 -1.3055  1.6701]\n",
      " [ 0.2433 -1.561   0.8805 -0.0295  0.9864  0.448   0.4326]\n",
      " [-1.2691 -0.5238 -0.5407 -1.1373 -0.2999  1.304  -1.7477]]\n",
      "res2= [[ 1.6953 -2.4214  0.5561  0.3529 -0.6678  0.8172  0.7055]\n",
      " [-1.5766 -1.3174 -1.0877  1.7223  1.1279 -1.3055  1.6701]\n",
      " [ 0.2433 -1.561   0.8805 -0.0295  0.9864  0.448   0.4326]\n",
      " [-1.2691 -0.5238 -0.5407 -1.1373 -0.2999  1.304  -1.7477]]\n"
     ]
    }
   ],
   "source": [
    "view_shape= out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img.strides+img.strides\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= np.lib.stride_tricks.as_strided(img, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "# print(f'sub_matrices*w= {sub_matrices*w}')\n",
    "res1= (sub_matrices*w).sum(axis=(2,3))\n",
    "print(f'res1= {res1}')\n",
    "res2= np.einsum('ijkl,ijkl->ij', w, sub_matrices)\n",
    "print(f'res2= {res2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fbeb2-8d60-49d5-9654-050b97d6dee9",
   "metadata": {},
   "source": [
    "### With Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "63614575-bd23-43ca-8f67-862f64b017bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (1, 1, 4, 7, 3, 3)\n",
      "strides= (54, 54, 9, 1, 9, 1)\n",
      "sub_matrices.shape= torch.Size([1, 1, 4, 7, 3, 3])\n",
      "res1= tensor([[[[ 1.6953, -2.4214,  0.5561,  0.3529, -0.6678,  0.8172,  0.7055],\n",
      "          [-1.5766, -1.3174, -1.0877,  1.7223,  1.1279, -1.3055,  1.6701],\n",
      "          [ 0.2433, -1.5610,  0.8805, -0.0295,  0.9864,  0.4480,  0.4326],\n",
      "          [-1.2691, -0.5238, -0.5407, -1.1373, -0.2999,  1.3040, -1.7477]]]])\n",
      "res2= tensor([[[[ 1.6953, -2.4214,  0.5561,  0.3529, -0.6678,  0.8172,  0.7055],\n",
      "          [-1.5766, -1.3174, -1.0877,  1.7223,  1.1279, -1.3055,  1.6701],\n",
      "          [ 0.2433, -1.5610,  0.8805, -0.0295,  0.9864,  0.4480,  0.4326],\n",
      "          [-1.2691, -0.5238, -0.5407, -1.1373, -0.2999,  1.3040, -1.7477]]]])\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).reshape((1,1,)+img.shape).float()\n",
    "w_torch= torch.from_numpy(w).reshape((1,1,)+w.shape).float()\n",
    "view_shape= (1,1,)+out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img_torch.stride()+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "res1= (sub_matrices*w_torch).sum(axis=(4,5))\n",
    "print(f'res1= {res1}')\n",
    "res2= torch.einsum('ijklmn,ijklmn->ijkl', w_torch, sub_matrices)\n",
    "print(f'res2= {res2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15034d0f-f584-4484-bb6d-9ea2b719ca1c",
   "metadata": {},
   "source": [
    "## Pixel-variant kernel (N-batch=1, N-inchannels=2, N-outchannels=3)\n",
    "Note: The following code works with N-batch>=2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f5c36328-eec4-41d5-b028-b8d71cc353a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[[[0.98 0.13 0.47 0.79 0.96 0.87 0.65 0.45 0.27]\n",
      "   [0.22 0.94 1.   0.39 0.41 0.7  0.98 0.95 0.64]\n",
      "   [0.82 0.94 0.75 0.48 0.7  0.42 0.91 0.3  0.82]\n",
      "   [0.94 0.42 0.02 0.8  0.63 0.36 0.68 0.71 0.93]\n",
      "   [0.44 0.55 0.94 0.22 0.06 0.9  0.69 0.86 0.24]\n",
      "   [0.86 0.46 0.65 0.13 0.25 0.69 0.26 0.54 0.14]]\n",
      "\n",
      "  [[0.11 0.43 0.89 0.6  0.   0.24 0.92 0.36 0.05]\n",
      "   [0.78 0.69 0.43 0.87 0.3  0.19 0.06 0.84 0.6 ]\n",
      "   [0.09 0.88 0.89 0.15 0.66 0.97 0.47 0.97 0.4 ]\n",
      "   [0.52 0.31 0.89 0.1  0.22 0.37 0.22 0.92 0.85]\n",
      "   [0.65 0.21 0.31 0.53 0.29 0.66 0.13 0.67 0.48]\n",
      "   [0.33 0.31 0.56 0.84 0.41 0.45 0.82 0.6  0.61]]]], (1, 2, 6, 9)\n",
      "w= [[[[[[ 0.36  0.26  0.67]\n",
      "     [-0.35  0.67  0.49]\n",
      "     [-0.13  0.26  0.62]]]]]]..., (3, 2, 4, 7, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "n_bch,in_ch,out_ch,ks= 1,2,3,3\n",
    "img= np.round(np.random.uniform(0,1,size=(n_bch,in_ch,6,9)),2)\n",
    "kernel_shape= (ks,ks)\n",
    "out_shape= tuple(np.subtract(img.shape[2:], kernel_shape)+1)\n",
    "w= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch)+out_shape+kernel_shape),2)\n",
    "print(f'img= {img}, {img.shape}')\n",
    "print(f'w= {w[0:1,0:1,0:1,0:1,:,:]}..., {w.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52920750-4d24-4689-9253-dc7715bfdeaa",
   "metadata": {},
   "source": [
    "### Test: comparison with torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c60cb497-124e-4063-abc0-26a40ed3fe7f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[[[0.72 0.12 0.27 0.56 0.25 0.86 0.9  0.18 0.46]\n",
      "   [0.56 0.12 0.72 0.28 0.79 0.21 0.61 0.75 0.96]\n",
      "   [0.3  0.67 0.49 0.28 0.84 0.06 0.42 0.7  0.43]\n",
      "   [0.13 0.62 0.24 0.1  0.05 0.59 0.43 0.41 0.62]\n",
      "   [0.69 0.75 0.12 0.09 0.62 0.95 0.78 0.6  0.88]\n",
      "   [0.15 0.86 0.43 0.78 0.6  0.18 0.25 0.34 0.63]]\n",
      "\n",
      "  [[0.34 0.19 0.52 0.19 0.25 0.97 0.71 0.98 0.97]\n",
      "   [0.73 0.56 0.74 0.81 0.37 0.48 0.4  0.39 0.32]\n",
      "   [0.78 0.37 0.78 0.4  0.39 0.67 0.27 0.3  0.17]\n",
      "   [0.15 0.81 0.4  0.11 0.38 0.61 0.78 0.37 0.58]\n",
      "   [0.56 0.47 0.54 0.11 0.39 0.94 0.16 0.75 0.12]\n",
      "   [0.66 0.43 0.54 0.02 0.41 0.49 0.41 0.23 0.15]]]], (1, 2, 6, 9)\n",
      "w= [[[[[[ 0.9   0.67  0.26]\n",
      "     [ 0.59  0.43 -0.64]\n",
      "     [-0.07  0.92 -0.77]]]]]]..., (3, 2, 4, 7, 3, 3), (3, 2, 1, 1, 3, 3)\n",
      "res_gt= tensor([[[[-0.3094, -0.5318, -0.6258,  0.2221, -1.0388, -1.1220, -1.0099],\n",
      "          [-1.3240, -0.4891, -0.8321, -1.0456,  0.1132, -0.8761,  0.5982],\n",
      "          [-0.1455, -0.1262, -0.7849, -1.3490, -0.5358,  0.0394, -0.5605],\n",
      "          [ 0.3784, -0.3125, -0.6410, -1.2007, -0.8468, -0.5885, -0.5655]],\n",
      "\n",
      "         [[-1.1018, -1.7314, -1.2620, -0.7722, -1.4841, -0.6868, -1.1290],\n",
      "          [-1.0938, -1.4458, -1.3368,  0.2741, -0.7131, -1.4829, -0.4159],\n",
      "          [-0.9903, -1.3955,  0.4276,  0.0124, -1.5761, -0.7714, -1.3666],\n",
      "          [-0.8113, -0.5234, -0.4199, -1.4058, -1.6079, -0.9959, -0.7517]],\n",
      "\n",
      "         [[ 1.3588,  1.3836,  1.3449,  0.7846,  1.0984, -0.7983, -0.3996],\n",
      "          [ 0.0515,  0.5154, -0.6156, -0.0457,  1.5047,  0.9693,  1.8315],\n",
      "          [ 1.0887,  0.3160,  0.5710,  0.5470,  1.5719,  2.2815,  1.4371],\n",
      "          [ 1.0358,  1.5201,  0.6427,  0.9880,  0.4165, -0.0291,  0.6963]]]])\n"
     ]
    }
   ],
   "source": [
    "w_base= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch)+(1,1)+kernel_shape),2)\n",
    "w= np.repeat(np.repeat(w_base,out_shape[1],axis=3),out_shape[0],axis=2)\n",
    "print(f'img= {img}, {img.shape}')\n",
    "print(f'w= {w[0:1,0:1,0:1,0:1,:,:]}..., {w.shape}, {w_base.shape}')\n",
    "img_torch= torch.from_numpy(img).float()\n",
    "w_base_torch= torch.from_numpy(w_base).reshape(w_base.shape[:2]+w_base.shape[4:]).float()\n",
    "# print(f'w_base_torch= {w_base_torch}..., {w_base_torch.shape}')\n",
    "res_gt= torch.nn.functional.conv2d(img_torch, w_base_torch)\n",
    "print(f'res_gt= {res_gt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb08e3-b59c-4ac1-bf0a-e8e3e72e2dc7",
   "metadata": {},
   "source": [
    "### With numpy stride_tricks.as_strided and einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5ecf43f9-8976-418b-838c-a97887c481e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (1, 2, 4, 7, 3, 3)\n",
      "strides= (864, 432, 72, 8, 72, 8)\n",
      "sub_matrices.shape= (1, 2, 4, 7, 3, 3)\n",
      "res_np= [[[[ 3.0778  1.7253 -0.5113 -0.0817 -0.581  -1.9058  0.7106]\n",
      "   [-1.9801  1.1653  0.3585 -1.7316  1.3211  0.6694  0.5069]\n",
      "   [ 0.6119  0.563  -0.7654  2.5973 -1.971   0.6882  0.8126]\n",
      "   [ 0.7336  0.9963  1.6773 -1.0295  1.2635  2.6903 -0.332 ]]\n",
      "\n",
      "  [[-1.364  -0.0372  1.4103 -0.1755  1.194  -0.2636  0.3186]\n",
      "   [-1.9607  0.4     1.9175 -1.7362  0.4124  1.0521 -0.605 ]\n",
      "   [-1.7676  2.4814  2.3001  0.1844 -0.6243  2.2404  0.7621]\n",
      "   [-1.7687 -0.2826  0.0179 -0.3561  0.365   0.8226  0.6107]]\n",
      "\n",
      "  [[-0.4571  1.1342  0.5213 -0.5604 -1.0119  0.4766  0.1784]\n",
      "   [ 0.0692 -0.2373 -1.4442 -0.3252 -2.1491  2.8538  0.116 ]\n",
      "   [-2.1918  0.3397  1.0173 -0.2993 -2.663   0.7206  0.1622]\n",
      "   [-0.5263  1.0427  0.5387  0.6553 -1.3975  0.8994 -1.7948]]]]\n"
     ]
    }
   ],
   "source": [
    "view_shape= img.shape[:2]+out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img.strides+img.strides[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= np.lib.stride_tricks.as_strided(img, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "# print(f'sub_matrices*w= {sub_matrices*w}')\n",
    "res_np= np.einsum('ijmnkl,bjmnkl->bimn', w, sub_matrices)\n",
    "print(f'res_np= {res_np}')\n",
    "# print(f'res_np= {res_np}, {torch.all(torch.isclose(res_gt,torch.from_numpy(res_np).float()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b7844-a1d5-437e-bf37-44d3d3aa87bb",
   "metadata": {},
   "source": [
    "### With Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9350cc0e-98ed-43cb-85c1-487a4fd5271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= torch.Size([1, 2, 4, 7, 3, 3])\n",
      "strides= (108, 54, 9, 1, 9, 1)\n",
      "sub_matrices.shape= torch.Size([1, 2, 4, 7, 3, 3])\n",
      "res_tch= tensor([[[[ 3.0778,  1.7253, -0.5113, -0.0817, -0.5810, -1.9058,  0.7106],\n",
      "          [-1.9801,  1.1653,  0.3585, -1.7316,  1.3211,  0.6694,  0.5069],\n",
      "          [ 0.6119,  0.5630, -0.7654,  2.5973, -1.9710,  0.6882,  0.8126],\n",
      "          [ 0.7336,  0.9963,  1.6773, -1.0295,  1.2635,  2.6903, -0.3320]],\n",
      "\n",
      "         [[-1.3640, -0.0372,  1.4103, -0.1755,  1.1940, -0.2636,  0.3186],\n",
      "          [-1.9607,  0.4000,  1.9175, -1.7362,  0.4124,  1.0521, -0.6050],\n",
      "          [-1.7676,  2.4814,  2.3001,  0.1844, -0.6243,  2.2404,  0.7621],\n",
      "          [-1.7687, -0.2826,  0.0179, -0.3561,  0.3650,  0.8226,  0.6107]],\n",
      "\n",
      "         [[-0.4571,  1.1342,  0.5213, -0.5604, -1.0119,  0.4766,  0.1784],\n",
      "          [ 0.0692, -0.2373, -1.4442, -0.3252, -2.1491,  2.8538,  0.1160],\n",
      "          [-2.1918,  0.3397,  1.0173, -0.2993, -2.6630,  0.7206,  0.1622],\n",
      "          [-0.5263,  1.0427,  0.5387,  0.6553, -1.3975,  0.8994, -1.7948]]]]), True\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "view_shape= img_torch.shape[:2]+out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img_torch.stride()+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "res_tch= torch.einsum('ijmnkl,bjmnkl->bimn', w_torch, sub_matrices)\n",
    "print(f'res_tch= {res_tch}, {torch.all(torch.isclose(res_tch,torch.from_numpy(res_np).float()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741d5b9-9bee-47af-a5b6-3284d9e877e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
