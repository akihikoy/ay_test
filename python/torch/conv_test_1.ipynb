{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ba914c-a94d-4971-b678-0021b27e8ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7df6df-a76b-43b4-9ea4-2948f7451823",
   "metadata": {},
   "source": [
    "# Implementing convolution from scratch\n",
    "Ref. https://stackoverflow.com/questions/43086557/convolve2d-just-by-using-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920c3e4-f673-4371-bcc2-77033874ab85",
   "metadata": {},
   "source": [
    "## Convolution (N-batch=1, N-inchannels=1, N-outchannels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7356c0ca-2530-44b5-86e3-45585c4c30e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[0.08 0.93 0.67 0.68 0.33 0.15 0.16 0.76 0.28]\n",
      " [0.48 0.57 0.15 0.68 0.49 0.69 0.39 0.99 0.54]\n",
      " [0.8  0.42 0.61 0.07 0.7  0.22 0.45 0.26 0.88]\n",
      " [0.11 0.35 0.65 0.08 0.65 0.52 0.23 0.86 0.9 ]\n",
      " [0.06 0.68 0.63 0.34 0.46 0.96 0.3  0.82 0.78]\n",
      " [0.22 0.88 0.9  0.02 0.26 0.14 0.57 0.45 0.23]]\n",
      "w= [[-0.92  0.33 -0.08]\n",
      " [-0.4   0.85 -0.37]\n",
      " [-0.91  0.31  0.42]]\n"
     ]
    }
   ],
   "source": [
    "img= np.round(np.random.uniform(0,1,size=(6,9)),2)\n",
    "w= np.round(np.random.uniform(-1,1,size=(3,3)),2)\n",
    "print(f'img= {img}')\n",
    "print(f'w= {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536dd380-7f17-4eb4-971d-7bd225670128",
   "metadata": {},
   "source": [
    "### Ground truth: torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "93292ee4-59a6-4700-af36-67171486edf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0751, -1.2047, -0.3211, -0.3938, -0.4005, -0.4083,  0.6076],\n",
       "          [-0.1728, -0.2881, -0.6900,  0.3136, -0.8475, -0.4277,  0.0754],\n",
       "          [-0.2124, -0.0885, -1.3013,  0.7135, -0.5055, -0.8416,  0.2162],\n",
       "          [ 0.7338, -0.4896, -1.4604,  0.1203,  0.1224, -0.6654,  0.0060]]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).reshape((1,1,)+img.shape).float()\n",
    "w_torch= torch.from_numpy(w).reshape((1,1,)+w.shape).float()\n",
    "torch.nn.functional.conv2d(img_torch, w_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9401f13d-f69a-43e6-b070-1b038a2a1330",
   "metadata": {},
   "source": [
    "### With numpy stride_tricks.as_strided and einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ff1aa6e4-7de3-41ca-b03c-40c8fe07ce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (4, 7, 3, 3)\n",
      "strides= (72, 8, 72, 8)\n",
      "conv1= [[ 0.0751 -1.2047 -0.3211 -0.3938 -0.4005 -0.4083  0.6076]\n",
      " [-0.1728 -0.2881 -0.69    0.3136 -0.8475 -0.4277  0.0754]\n",
      " [-0.2124 -0.0885 -1.3013  0.7135 -0.5055 -0.8416  0.2162]\n",
      " [ 0.7338 -0.4896 -1.4604  0.1203  0.1224 -0.6654  0.006 ]]\n",
      "conv2= [[ 0.0751 -1.2047 -0.3211 -0.3938 -0.4005 -0.4083  0.6076]\n",
      " [-0.1728 -0.2881 -0.69    0.3136 -0.8475 -0.4277  0.0754]\n",
      " [-0.2124 -0.0885 -1.3013  0.7135 -0.5055 -0.8416  0.2162]\n",
      " [ 0.7338 -0.4896 -1.4604  0.1203  0.1224 -0.6654  0.006 ]]\n"
     ]
    }
   ],
   "source": [
    "view_shape= tuple(np.subtract(img.shape, w.shape)+1)+w.shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img.strides+img.strides\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= np.lib.stride_tricks.as_strided(img, view_shape, strides)\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv1= (sub_matrices*w).sum(axis=(2,3))\n",
    "print(f'conv1= {conv1}')\n",
    "conv2= np.einsum('ij,klij->kl', w, sub_matrices)\n",
    "print(f'conv2= {conv2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120fffc-6081-44d4-9b02-28db84b15217",
   "metadata": {},
   "source": [
    "### With Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "347ebcc8-e7e9-4c46-9c82-c1b4acd30698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= torch.Size([1, 1, 4, 7, 3, 3])\n",
      "strides= (54, 54, 9, 1, 9, 1)\n",
      "conv1= tensor([[[[ 0.0751, -1.2047, -0.3211, -0.3938, -0.4005, -0.4083,  0.6076],\n",
      "          [-0.1728, -0.2881, -0.6900,  0.3136, -0.8475, -0.4277,  0.0754],\n",
      "          [-0.2124, -0.0885, -1.3013,  0.7135, -0.5055, -0.8416,  0.2162],\n",
      "          [ 0.7338, -0.4896, -1.4604,  0.1203,  0.1224, -0.6654,  0.0060]]]])\n",
      "conv2= tensor([[[[ 0.0751, -1.2047, -0.3211, -0.3938, -0.4005, -0.4083,  0.6076],\n",
      "          [-0.1728, -0.2881, -0.6900,  0.3136, -0.8475, -0.4277,  0.0754],\n",
      "          [-0.2124, -0.0885, -1.3013,  0.7135, -0.5055, -0.8416,  0.2162],\n",
      "          [ 0.7338, -0.4896, -1.4604,  0.1203,  0.1224, -0.6654,  0.0060]]]])\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).reshape((1,1,)+img.shape).float()\n",
    "w_torch= torch.from_numpy(w).reshape((1,1,)+w.shape).float()\n",
    "view_shape= img_torch.shape[:2]+tuple(np.subtract(img_torch.shape[2:],w_torch.shape[2:])+1)+w_torch.shape[2:]\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img_torch.stride()+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv1= (sub_matrices*w_torch).sum(axis=(4,5))\n",
    "print(f'conv1= {conv1}')\n",
    "conv2= torch.einsum('klij,klmnij->klmn', w_torch, sub_matrices)\n",
    "print(f'conv2= {conv2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c2ca1-daba-45a3-86fa-0a9b3dac9727",
   "metadata": {},
   "source": [
    "## Convolution (N-batch=1, N-inchannels=2, N-outchannels=3)\n",
    "Note: The following code works with n_bch>=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d7d118c7-147a-4c66-9d78-5e0a140de320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[[[0.58 0.38 0.12 0.17 0.97 0.18 0.95 0.95 0.27]\n",
      "   [0.09 0.86 0.63 0.99 0.84 0.24 0.21 0.26 0.21]\n",
      "   [0.89 0.43 0.9  0.75 0.87 0.16 0.41 0.08 0.11]\n",
      "   [0.16 0.81 0.15 0.56 0.35 0.11 0.41 0.9  0.09]\n",
      "   [0.73 0.43 0.98 0.79 0.52 0.54 0.15 0.25 0.19]\n",
      "   [0.99 0.19 0.04 0.77 0.36 0.47 0.14 0.52 0.15]]\n",
      "\n",
      "  [[0.89 0.98 0.89 0.14 0.78 0.14 0.95 0.95 0.73]\n",
      "   [0.12 0.13 0.87 0.72 0.27 0.96 0.91 0.84 0.14]\n",
      "   [0.37 0.56 0.06 0.5  0.27 0.07 0.57 0.86 0.44]\n",
      "   [0.46 0.13 0.26 0.31 0.9  0.86 0.93 0.24 0.58]\n",
      "   [0.84 0.62 0.58 0.91 0.86 0.59 0.16 0.31 0.32]\n",
      "   [0.39 0.44 0.67 0.37 0.96 0.83 0.26 0.91 0.58]]]]\n",
      "w= [[[[-1.   -0.12 -0.24]\n",
      "   [ 0.5   0.92  0.81]\n",
      "   [ 0.47  0.18 -0.04]]\n",
      "\n",
      "  [[-0.11  0.6   0.16]\n",
      "   [-0.19  0.49 -0.37]\n",
      "   [-0.09  0.44  0.28]]]\n",
      "\n",
      "\n",
      " [[[ 0.5  -0.2   0.45]\n",
      "   [-0.49 -0.34  0.54]\n",
      "   [-0.6   0.16 -0.82]]\n",
      "\n",
      "  [[ 0.42 -0.53 -0.91]\n",
      "   [-0.79  0.3   0.42]\n",
      "   [-0.93 -0.9  -0.54]]]\n",
      "\n",
      "\n",
      " [[[-0.69 -0.89  0.13]\n",
      "   [-0.94 -0.65  0.37]\n",
      "   [-0.46  0.03 -0.45]]\n",
      "\n",
      "  [[-0.3  -0.93  0.54]\n",
      "   [ 0.7  -0.01 -0.04]\n",
      "   [ 0.56  0.51  0.46]]]]\n"
     ]
    }
   ],
   "source": [
    "n_bch,in_ch,out_ch,ks= 1,2,3,3\n",
    "img= np.round(np.random.uniform(0,1,size=(n_bch,in_ch,6,9)),2)\n",
    "w= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch,ks,ks)),2)\n",
    "print(f'img= {img}')\n",
    "print(f'w= {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a9812-f4f6-4c01-ac00-dcf83ed6b3be",
   "metadata": {},
   "source": [
    "### Ground truth: torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "79a8fc8a-0b04-4a85-80e1-f9576816824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_gt= tensor([[[[ 1.7332,  2.4102,  2.5449,  1.8440,  0.4113,  1.2922,  0.8077],\n",
      "          [ 1.9147,  1.4130,  1.9298,  1.3351,  1.1848,  1.2055,  1.1881],\n",
      "          [ 0.7150,  1.1221,  0.9520,  0.8854,  0.1931,  1.9831,  1.2294],\n",
      "          [ 2.1038,  1.2396,  2.1700,  2.3855,  1.6488,  1.1085,  0.7497]],\n",
      "\n",
      "         [[-2.4579, -1.1591, -2.2823, -2.4235, -1.1120, -2.3014, -2.7134],\n",
      "          [-1.6237, -2.0818, -1.0713, -3.0737, -3.6110, -2.8510, -1.7890],\n",
      "          [-2.7452, -2.2581, -2.3141, -2.4440, -1.6706, -2.3933, -2.0621],\n",
      "          [-2.0524, -1.7402, -2.7161, -3.8014, -3.4150, -2.6239, -1.6516]],\n",
      "\n",
      "         [[-2.0643, -2.1252, -0.7881, -2.5149, -1.4129, -0.2671, -1.1814],\n",
      "          [-0.6381, -2.0971, -2.4688, -1.4834, -0.8247, -0.7548, -0.6436],\n",
      "          [-1.5008, -0.9920, -1.2218, -1.5451,  0.3733,  0.1928, -1.1368],\n",
      "          [-0.6915, -0.7645, -0.4683, -0.9015, -0.1823, -0.8607, -0.6381]]]])\n"
     ]
    }
   ],
   "source": [
    "# print(torch.nn.Conv2d(2,3,bias=False,kernel_size=(3,3)).weight.shape)\n",
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "conv_gt= torch.nn.functional.conv2d(img_torch, w_torch)\n",
    "print(f'conv_gt= {conv_gt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec7d75-eb22-4aa1-92f3-063a6bd3a0d5",
   "metadata": {},
   "source": [
    "### With numpy stride_tricks.as_strided and einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "af58ec7b-33f7-48ef-a4af-3fbcdcb58986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (1, 2, 4, 7, 3, 3)\n",
      "strides= (864, 432, 72, 8, 72, 8)\n",
      "sub_matrices.shape= (1, 2, 4, 7, 3, 3)\n",
      "w.shape= (3, 2, 3, 3)\n",
      "conv_np= [[[[ 1.7332  2.4102  2.5449  1.844   0.4113  1.2922  0.8077]\n",
      "   [ 1.9147  1.413   1.9298  1.3351  1.1848  1.2055  1.1881]\n",
      "   [ 0.715   1.1221  0.952   0.8854  0.1931  1.9831  1.2294]\n",
      "   [ 2.1038  1.2396  2.17    2.3855  1.6488  1.1085  0.7497]]\n",
      "\n",
      "  [[-2.4579 -1.1591 -2.2823 -2.4235 -1.112  -2.3014 -2.7134]\n",
      "   [-1.6237 -2.0818 -1.0713 -3.0737 -3.611  -2.851  -1.789 ]\n",
      "   [-2.7452 -2.2581 -2.3141 -2.444  -1.6706 -2.3933 -2.0621]\n",
      "   [-2.0524 -1.7402 -2.7161 -3.8014 -3.415  -2.6239 -1.6516]]\n",
      "\n",
      "  [[-2.0643 -2.1252 -0.7881 -2.5149 -1.4129 -0.2671 -1.1814]\n",
      "   [-0.6381 -2.0971 -2.4688 -1.4834 -0.8247 -0.7548 -0.6436]\n",
      "   [-1.5008 -0.992  -1.2218 -1.5451  0.3733  0.1928 -1.1368]\n",
      "   [-0.6915 -0.7645 -0.4683 -0.9015 -0.1823 -0.8607 -0.6381]]]], True\n"
     ]
    }
   ],
   "source": [
    "view_shape= img.shape[:2]+tuple(np.subtract(img.shape[2:], w.shape[-2:])+1)+w.shape[-2:]\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img.strides+img.strides[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= np.lib.stride_tricks.as_strided(img, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "print(f'w.shape= {w.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv_np= np.einsum('ijkl,bjmnkl->bimn', w, sub_matrices)\n",
    "print(f'conv_np= {conv_np}, {torch.all(torch.isclose(conv_gt,torch.from_numpy(conv_np).float()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdae7c-962f-43bf-9231-6565750764f2",
   "metadata": {},
   "source": [
    "### With Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "42f216ac-3e46-450f-9066-aaad9ed42199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= torch.Size([1, 2, 4, 7, 3, 3])\n",
      "strides= (108, 54, 9, 1, 9, 1)\n",
      "sub_matrices.shape= torch.Size([1, 2, 4, 7, 3, 3])\n",
      "conv_tch= tensor([[[[ 1.7332,  2.4102,  2.5449,  1.8440,  0.4113,  1.2922,  0.8077],\n",
      "          [ 1.9147,  1.4130,  1.9298,  1.3351,  1.1848,  1.2055,  1.1881],\n",
      "          [ 0.7150,  1.1221,  0.9520,  0.8854,  0.1931,  1.9831,  1.2294],\n",
      "          [ 2.1038,  1.2396,  2.1700,  2.3855,  1.6488,  1.1085,  0.7497]],\n",
      "\n",
      "         [[-2.4579, -1.1591, -2.2823, -2.4235, -1.1120, -2.3014, -2.7134],\n",
      "          [-1.6237, -2.0818, -1.0713, -3.0737, -3.6110, -2.8510, -1.7890],\n",
      "          [-2.7452, -2.2581, -2.3141, -2.4440, -1.6706, -2.3933, -2.0621],\n",
      "          [-2.0524, -1.7402, -2.7161, -3.8014, -3.4150, -2.6239, -1.6516]],\n",
      "\n",
      "         [[-2.0643, -2.1252, -0.7881, -2.5149, -1.4129, -0.2671, -1.1814],\n",
      "          [-0.6381, -2.0971, -2.4688, -1.4834, -0.8247, -0.7548, -0.6436],\n",
      "          [-1.5008, -0.9920, -1.2218, -1.5451,  0.3733,  0.1928, -1.1368],\n",
      "          [-0.6915, -0.7645, -0.4683, -0.9015, -0.1823, -0.8607, -0.6381]]]]), True\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "view_shape= img_torch.shape[:2]+tuple(np.subtract(img_torch.shape[2:],w_torch.shape[-2:])+1)+w_torch.shape[-2:]\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img_torch.stride()+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv_tch= torch.einsum('ijkl,bjmnkl->bimn', w_torch, sub_matrices)\n",
    "print(f'conv_tch= {conv_tch}, {torch.all(torch.isclose(conv_gt,conv_tch))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f6166-0c76-4765-92ef-5cef589f2084",
   "metadata": {},
   "source": [
    "## Pixel-variant kernel (N-batch=1, N-inchannels=1, N-outchannels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "53ca54bd-6add-4f19-993c-967023f26bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[0.78 0.79 0.32 0.44 0.79 0.26 0.59 0.34 0.38]\n",
      " [0.94 0.62 0.43 0.03 0.82 0.45 0.63 0.64 0.43]\n",
      " [0.67 0.69 0.81 0.63 0.37 0.91 0.35 0.36 0.56]\n",
      " [0.02 0.74 0.88 0.68 0.51 0.15 0.41 0.68 0.29]\n",
      " [0.58 0.9  0.23 0.1  0.26 0.06 0.35 0.13 0.84]\n",
      " [0.57 0.67 0.11 0.6  0.3  0.58 0.75 0.15 0.06]]\n",
      "w= [[[[ 0.12  0.07  0.34]\n",
      "   [ 0.07  0.27  0.67]\n",
      "   [ 0.55 -0.11  0.77]]]]..., (4, 7, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "kernel_shape= (3,3)\n",
    "img= np.round(np.random.uniform(0,1,size=(6,9)),2)\n",
    "out_shape= tuple(np.subtract(img.shape, kernel_shape)+1)\n",
    "w= np.round(np.random.uniform(-1,1,size=out_shape+kernel_shape),2)\n",
    "print(f'img= {img}')\n",
    "print(f'w= {w[0:1,0:1,:,:]}..., {w.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d51887-b54f-4531-92f3-e95ce9b5de55",
   "metadata": {},
   "source": [
    "### With numpy stride_tricks.as_strided and einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a129166b-305b-497d-897d-bde33ca8ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (4, 7, 3, 3)\n",
      "strides= (72, 8, 72, 8)\n",
      "sub_matrices.shape= (4, 7, 3, 3)\n",
      "res1= [[ 1.6953 -2.4214  0.5561  0.3529 -0.6678  0.8172  0.7055]\n",
      " [-1.5766 -1.3174 -1.0877  1.7223  1.1279 -1.3055  1.6701]\n",
      " [ 0.2433 -1.561   0.8805 -0.0295  0.9864  0.448   0.4326]\n",
      " [-1.2691 -0.5238 -0.5407 -1.1373 -0.2999  1.304  -1.7477]]\n",
      "res2= [[ 1.6953 -2.4214  0.5561  0.3529 -0.6678  0.8172  0.7055]\n",
      " [-1.5766 -1.3174 -1.0877  1.7223  1.1279 -1.3055  1.6701]\n",
      " [ 0.2433 -1.561   0.8805 -0.0295  0.9864  0.448   0.4326]\n",
      " [-1.2691 -0.5238 -0.5407 -1.1373 -0.2999  1.304  -1.7477]]\n"
     ]
    }
   ],
   "source": [
    "view_shape= out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img.strides+img.strides\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= np.lib.stride_tricks.as_strided(img, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "# print(f'sub_matrices*w= {sub_matrices*w}')\n",
    "res1= (sub_matrices*w).sum(axis=(2,3))\n",
    "print(f'res1= {res1}')\n",
    "res2= np.einsum('ijkl,ijkl->ij', w, sub_matrices)\n",
    "print(f'res2= {res2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472deff-b5bf-4850-9eb6-101cbf80928e",
   "metadata": {},
   "source": [
    "### With Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c13dd56d-8aa2-4dac-9722-1041b2c443c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (1, 1, 4, 7, 3, 3)\n",
      "strides= (54, 54, 9, 1, 9, 1)\n",
      "sub_matrices.shape= torch.Size([1, 1, 4, 7, 3, 3])\n",
      "res1= tensor([[[[ 1.6953, -2.4214,  0.5561,  0.3529, -0.6678,  0.8172,  0.7055],\n",
      "          [-1.5766, -1.3174, -1.0877,  1.7223,  1.1279, -1.3055,  1.6701],\n",
      "          [ 0.2433, -1.5610,  0.8805, -0.0295,  0.9864,  0.4480,  0.4326],\n",
      "          [-1.2691, -0.5238, -0.5407, -1.1373, -0.2999,  1.3040, -1.7477]]]])\n",
      "res2= tensor([[[[ 1.6953, -2.4214,  0.5561,  0.3529, -0.6678,  0.8172,  0.7055],\n",
      "          [-1.5766, -1.3174, -1.0877,  1.7223,  1.1279, -1.3055,  1.6701],\n",
      "          [ 0.2433, -1.5610,  0.8805, -0.0295,  0.9864,  0.4480,  0.4326],\n",
      "          [-1.2691, -0.5238, -0.5407, -1.1373, -0.2999,  1.3040, -1.7477]]]])\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).reshape((1,1,)+img.shape).float()\n",
    "w_torch= torch.from_numpy(w).reshape((1,1,)+w.shape).float()\n",
    "view_shape= (1,1,)+out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img_torch.stride()+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "res1= (sub_matrices*w_torch).sum(axis=(4,5))\n",
    "print(f'res1= {res1}')\n",
    "res2= torch.einsum('ijklmn,ijklmn->ijkl', w_torch, sub_matrices)\n",
    "print(f'res2= {res2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320ef25-0e1c-485b-b1d0-a470cc8447cd",
   "metadata": {},
   "source": [
    "## Pixel-variant kernel (N-batch=1, N-inchannels=2, N-outchannels=3)\n",
    "Note: The following code works with N-batch>=2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3ea5d24f-b70b-462a-afbd-df061f8e6e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[[[0.98 0.13 0.47 0.79 0.96 0.87 0.65 0.45 0.27]\n",
      "   [0.22 0.94 1.   0.39 0.41 0.7  0.98 0.95 0.64]\n",
      "   [0.82 0.94 0.75 0.48 0.7  0.42 0.91 0.3  0.82]\n",
      "   [0.94 0.42 0.02 0.8  0.63 0.36 0.68 0.71 0.93]\n",
      "   [0.44 0.55 0.94 0.22 0.06 0.9  0.69 0.86 0.24]\n",
      "   [0.86 0.46 0.65 0.13 0.25 0.69 0.26 0.54 0.14]]\n",
      "\n",
      "  [[0.11 0.43 0.89 0.6  0.   0.24 0.92 0.36 0.05]\n",
      "   [0.78 0.69 0.43 0.87 0.3  0.19 0.06 0.84 0.6 ]\n",
      "   [0.09 0.88 0.89 0.15 0.66 0.97 0.47 0.97 0.4 ]\n",
      "   [0.52 0.31 0.89 0.1  0.22 0.37 0.22 0.92 0.85]\n",
      "   [0.65 0.21 0.31 0.53 0.29 0.66 0.13 0.67 0.48]\n",
      "   [0.33 0.31 0.56 0.84 0.41 0.45 0.82 0.6  0.61]]]], (1, 2, 6, 9)\n",
      "w= [[[[[[ 0.36  0.26  0.67]\n",
      "     [-0.35  0.67  0.49]\n",
      "     [-0.13  0.26  0.62]]]]]]..., (3, 2, 4, 7, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "n_bch,in_ch,out_ch,ks= 1,2,3,3\n",
    "img= np.round(np.random.uniform(0,1,size=(n_bch,in_ch,6,9)),2)\n",
    "kernel_shape= (ks,ks)\n",
    "out_shape= tuple(np.subtract(img.shape[2:], kernel_shape)+1)\n",
    "w= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch)+out_shape+kernel_shape),2)\n",
    "print(f'img= {img}, {img.shape}')\n",
    "print(f'w= {w[0:1,0:1,0:1,0:1,:,:]}..., {w.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52c059-20b2-4d09-a79d-5d0dae08bf99",
   "metadata": {},
   "source": [
    "### Test: comparison with torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "26b502e5-239c-464d-82a5-f9d958f218aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[[[0.72 0.12 0.27 0.56 0.25 0.86 0.9  0.18 0.46]\n",
      "   [0.56 0.12 0.72 0.28 0.79 0.21 0.61 0.75 0.96]\n",
      "   [0.3  0.67 0.49 0.28 0.84 0.06 0.42 0.7  0.43]\n",
      "   [0.13 0.62 0.24 0.1  0.05 0.59 0.43 0.41 0.62]\n",
      "   [0.69 0.75 0.12 0.09 0.62 0.95 0.78 0.6  0.88]\n",
      "   [0.15 0.86 0.43 0.78 0.6  0.18 0.25 0.34 0.63]]\n",
      "\n",
      "  [[0.34 0.19 0.52 0.19 0.25 0.97 0.71 0.98 0.97]\n",
      "   [0.73 0.56 0.74 0.81 0.37 0.48 0.4  0.39 0.32]\n",
      "   [0.78 0.37 0.78 0.4  0.39 0.67 0.27 0.3  0.17]\n",
      "   [0.15 0.81 0.4  0.11 0.38 0.61 0.78 0.37 0.58]\n",
      "   [0.56 0.47 0.54 0.11 0.39 0.94 0.16 0.75 0.12]\n",
      "   [0.66 0.43 0.54 0.02 0.41 0.49 0.41 0.23 0.15]]]], (1, 2, 6, 9)\n",
      "w= [[[[[[ 0.9   0.67  0.26]\n",
      "     [ 0.59  0.43 -0.64]\n",
      "     [-0.07  0.92 -0.77]]]]]]..., (3, 2, 4, 7, 3, 3), (3, 2, 1, 1, 3, 3)\n",
      "res_gt= tensor([[[[-0.3094, -0.5318, -0.6258,  0.2221, -1.0388, -1.1220, -1.0099],\n",
      "          [-1.3240, -0.4891, -0.8321, -1.0456,  0.1132, -0.8761,  0.5982],\n",
      "          [-0.1455, -0.1262, -0.7849, -1.3490, -0.5358,  0.0394, -0.5605],\n",
      "          [ 0.3784, -0.3125, -0.6410, -1.2007, -0.8468, -0.5885, -0.5655]],\n",
      "\n",
      "         [[-1.1018, -1.7314, -1.2620, -0.7722, -1.4841, -0.6868, -1.1290],\n",
      "          [-1.0938, -1.4458, -1.3368,  0.2741, -0.7131, -1.4829, -0.4159],\n",
      "          [-0.9903, -1.3955,  0.4276,  0.0124, -1.5761, -0.7714, -1.3666],\n",
      "          [-0.8113, -0.5234, -0.4199, -1.4058, -1.6079, -0.9959, -0.7517]],\n",
      "\n",
      "         [[ 1.3588,  1.3836,  1.3449,  0.7846,  1.0984, -0.7983, -0.3996],\n",
      "          [ 0.0515,  0.5154, -0.6156, -0.0457,  1.5047,  0.9693,  1.8315],\n",
      "          [ 1.0887,  0.3160,  0.5710,  0.5470,  1.5719,  2.2815,  1.4371],\n",
      "          [ 1.0358,  1.5201,  0.6427,  0.9880,  0.4165, -0.0291,  0.6963]]]])\n"
     ]
    }
   ],
   "source": [
    "w_base= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch)+(1,1)+kernel_shape),2)\n",
    "w= np.repeat(np.repeat(w_base,out_shape[1],axis=3),out_shape[0],axis=2)\n",
    "print(f'img= {img}, {img.shape}')\n",
    "print(f'w= {w[0:1,0:1,0:1,0:1,:,:]}..., {w.shape}, {w_base.shape}')\n",
    "img_torch= torch.from_numpy(img).float()\n",
    "w_base_torch= torch.from_numpy(w_base).reshape(w_base.shape[:2]+w_base.shape[4:]).float()\n",
    "# print(f'w_base_torch= {w_base_torch}..., {w_base_torch.shape}')\n",
    "res_gt= torch.nn.functional.conv2d(img_torch, w_base_torch)\n",
    "print(f'res_gt= {res_gt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf268af-769e-4519-9a46-4a25e0ff641f",
   "metadata": {},
   "source": [
    "### With numpy stride_tricks.as_strided and einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ff2a2ff2-1493-459e-9760-0f0e35940281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= (1, 2, 4, 7, 3, 3)\n",
      "strides= (864, 432, 72, 8, 72, 8)\n",
      "sub_matrices.shape= (1, 2, 4, 7, 3, 3)\n",
      "res_np= [[[[ 3.0778  1.7253 -0.5113 -0.0817 -0.581  -1.9058  0.7106]\n",
      "   [-1.9801  1.1653  0.3585 -1.7316  1.3211  0.6694  0.5069]\n",
      "   [ 0.6119  0.563  -0.7654  2.5973 -1.971   0.6882  0.8126]\n",
      "   [ 0.7336  0.9963  1.6773 -1.0295  1.2635  2.6903 -0.332 ]]\n",
      "\n",
      "  [[-1.364  -0.0372  1.4103 -0.1755  1.194  -0.2636  0.3186]\n",
      "   [-1.9607  0.4     1.9175 -1.7362  0.4124  1.0521 -0.605 ]\n",
      "   [-1.7676  2.4814  2.3001  0.1844 -0.6243  2.2404  0.7621]\n",
      "   [-1.7687 -0.2826  0.0179 -0.3561  0.365   0.8226  0.6107]]\n",
      "\n",
      "  [[-0.4571  1.1342  0.5213 -0.5604 -1.0119  0.4766  0.1784]\n",
      "   [ 0.0692 -0.2373 -1.4442 -0.3252 -2.1491  2.8538  0.116 ]\n",
      "   [-2.1918  0.3397  1.0173 -0.2993 -2.663   0.7206  0.1622]\n",
      "   [-0.5263  1.0427  0.5387  0.6553 -1.3975  0.8994 -1.7948]]]]\n"
     ]
    }
   ],
   "source": [
    "view_shape= img.shape[:2]+out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img.strides+img.strides[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= np.lib.stride_tricks.as_strided(img, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "# print(f'sub_matrices*w= {sub_matrices*w}')\n",
    "res_np= np.einsum('ijmnkl,bjmnkl->bimn', w, sub_matrices)\n",
    "print(f'res_np= {res_np}')\n",
    "# print(f'res_np= {res_np}, {torch.all(torch.isclose(res_gt,torch.from_numpy(res_np).float()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb30a6-dd9a-42b8-8bb9-4f969e3c397d",
   "metadata": {},
   "source": [
    "### With Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "40c214ce-55d9-4a2b-af2b-f1c4287d4838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= torch.Size([1, 2, 4, 7, 3, 3])\n",
      "strides= (108, 54, 9, 1, 9, 1)\n",
      "sub_matrices.shape= torch.Size([1, 2, 4, 7, 3, 3])\n",
      "res_tch= tensor([[[[ 3.0778,  1.7253, -0.5113, -0.0817, -0.5810, -1.9058,  0.7106],\n",
      "          [-1.9801,  1.1653,  0.3585, -1.7316,  1.3211,  0.6694,  0.5069],\n",
      "          [ 0.6119,  0.5630, -0.7654,  2.5973, -1.9710,  0.6882,  0.8126],\n",
      "          [ 0.7336,  0.9963,  1.6773, -1.0295,  1.2635,  2.6903, -0.3320]],\n",
      "\n",
      "         [[-1.3640, -0.0372,  1.4103, -0.1755,  1.1940, -0.2636,  0.3186],\n",
      "          [-1.9607,  0.4000,  1.9175, -1.7362,  0.4124,  1.0521, -0.6050],\n",
      "          [-1.7676,  2.4814,  2.3001,  0.1844, -0.6243,  2.2404,  0.7621],\n",
      "          [-1.7687, -0.2826,  0.0179, -0.3561,  0.3650,  0.8226,  0.6107]],\n",
      "\n",
      "         [[-0.4571,  1.1342,  0.5213, -0.5604, -1.0119,  0.4766,  0.1784],\n",
      "          [ 0.0692, -0.2373, -1.4442, -0.3252, -2.1491,  2.8538,  0.1160],\n",
      "          [-2.1918,  0.3397,  1.0173, -0.2993, -2.6630,  0.7206,  0.1622],\n",
      "          [-0.5263,  1.0427,  0.5387,  0.6553, -1.3975,  0.8994, -1.7948]]]]), True\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "view_shape= img_torch.shape[:2]+out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "strides= img_torch.stride()+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "res_tch= torch.einsum('ijmnkl,bjmnkl->bimn', w_torch, sub_matrices)\n",
    "print(f'res_tch= {res_tch}, {torch.all(torch.isclose(res_tch,torch.from_numpy(res_np).float()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a330102-2a78-416f-ae53-3f2fc627b94b",
   "metadata": {},
   "source": [
    "# Advanced version (with padding, stride, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885004f6-03ab-41b9-bb7a-98912937b060",
   "metadata": {},
   "source": [
    "## Convolution (N-batch=1, N-inchannels=2, N-outchannels=3, padding=(ks-1)//2, stride=2)\n",
    "Note: The following code works with n_bch>=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b20c9cdf-4bb9-4809-bbda-a1d44d62acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[[[0.97 0.88 0.33 0.56 0.96 0.28 0.76 0.06 0.02 0.62]\n",
      "   [0.1  0.5  0.85 0.32 0.25 0.31 0.1  0.96 0.6  0.75]\n",
      "   [0.03 0.72 0.6  0.81 0.95 0.67 0.24 0.73 0.74 0.78]\n",
      "   [0.9  0.55 0.08 0.91 0.22 0.16 0.16 0.73 0.02 0.11]\n",
      "   [0.4  0.27 0.73 0.86 0.25 0.26 0.38 0.37 0.11 0.71]\n",
      "   [0.84 0.44 0.03 0.76 0.25 0.95 0.65 0.8  0.15 0.78]]\n",
      "\n",
      "  [[0.52 0.01 0.   0.05 0.77 0.86 1.   0.71 0.43 0.3 ]\n",
      "   [0.36 0.89 0.47 0.79 0.23 0.06 0.84 0.39 0.83 0.08]\n",
      "   [0.27 0.96 0.15 0.31 0.67 0.51 0.91 0.14 0.09 0.41]\n",
      "   [0.01 0.32 0.14 0.55 0.49 0.29 0.43 0.35 0.59 0.34]\n",
      "   [0.47 0.98 0.97 0.4  0.58 0.25 0.47 0.82 0.89 0.77]\n",
      "   [0.54 0.44 0.2  0.2  0.72 0.37 0.75 0.91 0.65 0.64]]]] ((1, 2, 6, 10))\n",
      "w= [[[[-0.25 -0.42  0.4 ]\n",
      "   [-0.49  0.75 -0.85]\n",
      "   [ 0.45  0.22 -0.62]]\n",
      "\n",
      "  [[ 0.58  0.37  0.56]\n",
      "   [-0.02  0.58  0.63]\n",
      "   [-0.13 -0.48  0.81]]]\n",
      "\n",
      "\n",
      " [[[ 0.4  -0.87  0.65]\n",
      "   [ 0.39  0.09 -0.22]\n",
      "   [-0.89 -0.11 -0.4 ]]\n",
      "\n",
      "  [[-0.18 -0.13  0.32]\n",
      "   [ 0.26 -0.46  0.47]\n",
      "   [-0.82 -0.16 -0.8 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.77 -0.78 -0.97]\n",
      "   [-0.3   0.01 -0.9 ]\n",
      "   [ 0.57  0.25  0.92]]\n",
      "\n",
      "  [[-0.37  0.01  0.93]\n",
      "   [-0.35  0.54  0.87]\n",
      "   [-0.92  0.07 -0.42]]]] ((3, 2, 3, 3))\n",
      "b= [0.71 0.44 0.11] ((3,))\n"
     ]
    }
   ],
   "source": [
    "n_bch,in_ch,out_ch,ks,stride= 1,2,3,3,2\n",
    "padding=(ks-1)//2\n",
    "padding_mode='zeros'\n",
    "valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "factory_kwargs= {'device': None, 'dtype': None}\n",
    "img= np.round(np.random.uniform(0,1,size=(n_bch,in_ch,6,10)),2)\n",
    "w= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch,ks,ks)),2)\n",
    "b= np.round(np.random.uniform(-1,1,size=(out_ch,)),2)\n",
    "# self.bias= Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "# self.register_parameter('bias', None)\n",
    "print(f'img= {img} ({img.shape})')\n",
    "print(f'w= {w} ({w.shape})')\n",
    "print(f'b= {b} ({b.shape})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3b9b1-11ca-4283-90c4-4782a0e1e506",
   "metadata": {},
   "source": [
    "### Ground truth: torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a2df47b-faef-4eaf-bbe0-122cbc59e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_gt= tensor([[[[ 1.2575,  0.5938,  1.7473,  1.5731,  0.3075],\n",
      "          [ 1.7829,  1.1979,  1.9626,  1.0695,  1.1495],\n",
      "          [ 1.7046,  1.6697,  0.8839,  2.4489,  1.4549]],\n",
      "\n",
      "         [[-0.8814, -1.3877, -0.4227, -0.4648, -1.2804],\n",
      "          [ 0.5107, -0.9642, -0.9943, -0.0307, -0.3449],\n",
      "          [-0.3702,  0.0088, -0.7570, -1.2568, -1.4838]],\n",
      "\n",
      "         [[-0.2464, -0.9405,  0.6403,  1.7610,  0.8318],\n",
      "          [ 1.3089, -0.3375, -0.3856, -0.3134, -0.8664],\n",
      "          [ 0.5074,  0.1467,  1.5369,  0.8652,  1.0076]]]]) (torch.Size([1, 3, 3, 5]))\n"
     ]
    }
   ],
   "source": [
    "# print(torch.nn.Conv2d(in_ch,out_ch,padding=padding,stride=stride,kernel_size=(ks,ks),bias=True).weight.shape)\n",
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "b_torch= torch.from_numpy(b).float()\n",
    "stride_= torch.nn.modules.utils._pair(stride)\n",
    "padding_= torch.nn.modules.utils._pair(padding)\n",
    "assert(padding_mode in valid_padding_modes)\n",
    "# valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "# factory_kwargs= {'device': None, 'dtype': None}\n",
    "if padding_mode=='zeros':\n",
    "  conv_gt= torch.nn.functional.conv2d(img_torch, w_torch, b_torch, stride_, padding_)\n",
    "else:\n",
    "  img_torch= torch.nn.functional.pad(img_torch, tuple(x for x in reversed(padding_) for _ in range(2)), mode=padding_mode)\n",
    "  conv_gt= torch.nn.functional.conv2d(img_torch, w_torch, b_torch, stride_, torch.nn.modules.utils._pair(0))\n",
    "print(f'conv_gt= {conv_gt} ({conv_gt.shape})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddc2ea-8ce7-4bbb-9257-25b25fc6ad0c",
   "metadata": {},
   "source": [
    "### With Torch (wo torch.conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "465f05a1-f3bf-4e98-a1b3-db2f987f5bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= torch.Size([1, 2, 3, 5, 3, 3])\n",
      "strides= (192, 96, 24, 2, 12, 1)\n",
      "sub_matrices.shape= torch.Size([1, 2, 3, 5, 3, 3])\n",
      "conv_tch= tensor([[[[ 1.2575,  0.5938,  1.7473,  1.5731,  0.3075],\n",
      "          [ 1.7829,  1.1979,  1.9626,  1.0695,  1.1495],\n",
      "          [ 1.7046,  1.6697,  0.8839,  2.4489,  1.4549]],\n",
      "\n",
      "         [[-0.8814, -1.3877, -0.4227, -0.4648, -1.2804],\n",
      "          [ 0.5107, -0.9642, -0.9943, -0.0307, -0.3449],\n",
      "          [-0.3702,  0.0088, -0.7570, -1.2568, -1.4838]],\n",
      "\n",
      "         [[-0.2464, -0.9405,  0.6403,  1.7610,  0.8318],\n",
      "          [ 1.3089, -0.3375, -0.3856, -0.3134, -0.8664],\n",
      "          [ 0.5074,  0.1467,  1.5369,  0.8652,  1.0076]]]]) (torch.Size([1, 3, 3, 5])), True\n"
     ]
    }
   ],
   "source": [
    "# print(torch.nn.Conv2d(2,3,bias=False,kernel_size=(3,3)).weight.shape)\n",
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "b_torch= torch.from_numpy(b).float()\n",
    "stride_= torch.nn.modules.utils._pair(stride)\n",
    "padding_= torch.nn.modules.utils._pair(padding)\n",
    "assert(padding_mode in valid_padding_modes)\n",
    "# valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "view_h= int((img_torch.shape[2]+2*padding_[0]-1*(w_torch.shape[-2]-1)-1)/stride_[0]+1)\n",
    "view_w= int((img_torch.shape[3]+2*padding_[1]-1*(w_torch.shape[-1]-1)-1)/stride_[1]+1)\n",
    "view_shape= img_torch.shape[:2]+(view_h,view_w)+w_torch.shape[-2:]\n",
    "print(f'view_shape= {view_shape}')\n",
    "if padding_mode=='zeros':\n",
    "  img_torch= torch.nn.functional.pad(img_torch, tuple(x for x in reversed(padding_) for _ in range(2)), mode='constant', value=0.0)\n",
    "else:\n",
    "  img_torch= torch.nn.functional.pad(img_torch, tuple(x for x in reversed(padding_) for _ in range(2)), mode=padding_mode)\n",
    "\n",
    "strides= img_torch.stride()[:2]+tuple(np.array(img_torch.stride()[2:])*stride_)+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "conv_tch= torch.einsum('ijkl,bjmnkl->bimn', w_torch, sub_matrices)+b_torch.reshape((-1,1,1))\n",
    "print(f'conv_tch= {conv_tch} ({conv_tch.shape}), {torch.all(torch.isclose(conv_gt,conv_tch))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e2f44-9fe2-44c1-ba3b-33092f01d56b",
   "metadata": {},
   "source": [
    "## Pixel-variant kernel (N-batch=1, N-inchannels=2, N-outchannels=3, padding=(ks-1)//2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e0d1afd8-f768-4bdb-8a83-b4fcd226aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img= [[[[0.13 0.78 0.13 0.46 0.61 0.87 0.13 0.55 0.95 0.35]\n",
      "   [0.1  0.03 0.73 0.81 0.12 0.3  0.51 0.13 0.86 0.95]\n",
      "   [0.93 0.3  0.85 0.47 0.38 0.36 0.92 0.43 0.63 0.25]\n",
      "   [0.93 0.04 0.81 0.17 0.29 0.9  0.47 0.48 0.43 0.66]\n",
      "   [0.8  0.35 0.97 0.59 0.23 0.72 0.81 0.09 0.5  0.51]\n",
      "   [0.57 0.73 0.67 0.61 0.51 0.57 0.66 0.13 0.61 0.94]]\n",
      "\n",
      "  [[0.87 0.19 0.36 0.82 0.91 0.79 0.34 0.95 0.36 0.98]\n",
      "   [1.   0.54 0.34 0.78 0.02 0.78 1.   0.92 0.73 0.6 ]\n",
      "   [0.21 0.38 0.18 0.91 0.34 0.74 0.37 0.85 0.79 0.83]\n",
      "   [0.17 0.28 0.27 0.44 0.5  0.81 0.25 0.16 0.97 0.72]\n",
      "   [0.51 0.54 0.22 0.69 0.71 0.06 0.58 0.19 0.94 0.57]\n",
      "   [0.85 0.86 0.53 0.22 0.25 0.54 0.26 0.95 0.51 0.46]]]], (1, 2, 6, 10)\n",
      "w= [[[[[[ 0.16 -0.65  0.45]\n",
      "     [ 1.    0.82  0.01]\n",
      "     [ 0.51  0.06  0.14]]]]]]..., (3, 2, 3, 5, 3, 3)\n",
      "b= [[[ 0.73  0.63  0.63 -0.83  0.54]\n",
      "  [ 0.3  -0.38 -0.97 -0.35 -0.15]\n",
      "  [ 0.24 -0.31  0.02  0.65  0.82]]\n",
      "\n",
      " [[ 0.65 -0.18  0.84  0.03  0.19]\n",
      "  [-0.64  0.51 -0.69  0.76 -0.88]\n",
      "  [ 0.24 -0.21  0.42  0.37  0.6 ]]\n",
      "\n",
      " [[ 0.28  0.36 -0.65  0.98  0.99]\n",
      "  [ 0.4  -0.17  0.86  0.23  0.7 ]\n",
      "  [-0.23 -0.6  -0.43  0.73 -0.05]]]..., (3, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "n_bch,in_ch,out_ch,ks,stride= 1,2,3,3,2\n",
    "padding=(ks-1)//2\n",
    "padding_mode='zeros'\n",
    "valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "factory_kwargs= {'device': None, 'dtype': None}\n",
    "img= np.round(np.random.uniform(0,1,size=(n_bch,in_ch,6,10)),2)\n",
    "kernel_shape= (ks,ks)\n",
    "stride_= torch.nn.modules.utils._pair(stride)\n",
    "padding_= torch.nn.modules.utils._pair(padding)\n",
    "out_h= int((img.shape[2]+2*padding_[0]-1*(kernel_shape[0]-1)-1)/stride_[0]+1)\n",
    "out_w= int((img.shape[3]+2*padding_[1]-1*(kernel_shape[1]-1)-1)/stride_[1]+1)\n",
    "out_shape= (out_h,out_w)\n",
    "w= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch)+out_shape+kernel_shape),2)\n",
    "b= np.round(np.random.uniform(-1,1,size=(out_ch,)+out_shape),2)\n",
    "print(f'img= {img}, {img.shape}')\n",
    "print(f'w= {w[0:1,0:1,0:1,0:1,:,:]}..., {w.shape}')\n",
    "print(f'b= {b}..., {b.shape}')\n",
    "res_gt= None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde39af-89c1-491c-9644-5045f1fde80d",
   "metadata": {},
   "source": [
    "### Test: comparison with torch.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f410628f-f3dd-40a9-82be-0e4effd3001c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= [[[[[[ 0.03 -0.9  -0.43]\n",
      "     [ 0.22 -0.47 -0.62]\n",
      "     [-0.01 -0.74 -0.1 ]]]]]]..., (3, 2, 3, 5, 3, 3), (3, 2, 1, 1, 3, 3)\n",
      "b= [[[ 0.22  0.22  0.22  0.22  0.22]\n",
      "  [ 0.22  0.22  0.22  0.22  0.22]\n",
      "  [ 0.22  0.22  0.22  0.22  0.22]]\n",
      "\n",
      " [[ 0.65  0.65  0.65  0.65  0.65]\n",
      "  [ 0.65  0.65  0.65  0.65  0.65]\n",
      "  [ 0.65  0.65  0.65  0.65  0.65]]\n",
      "\n",
      " [[-0.58 -0.58 -0.58 -0.58 -0.58]\n",
      "  [-0.58 -0.58 -0.58 -0.58 -0.58]\n",
      "  [-0.58 -0.58 -0.58 -0.58 -0.58]]]..., (3, 3, 5), (3, 1, 1)\n",
      "res_gt= tensor([[[[-1.0330, -1.0288, -1.2870, -1.1536, -1.1331],\n",
      "          [-1.8166, -3.0955, -0.9112, -2.0081, -3.2835],\n",
      "          [-1.6533, -2.0332, -2.4586, -2.3034, -3.2587]],\n",
      "\n",
      "         [[ 0.5351,  1.2488,  2.7257,  0.8186,  1.9153],\n",
      "          [-0.4047, -0.5823,  0.7149,  0.8679, -0.3799],\n",
      "          [-0.6785, -1.2071, -0.0506,  0.7773, -1.4025]],\n",
      "\n",
      "         [[ 0.6793, -0.2399, -0.3794,  0.3111,  0.5520],\n",
      "          [-0.2498, -1.8309, -1.8522, -0.1971, -2.2337],\n",
      "          [-0.8209, -2.7957, -0.3283, -1.4841, -2.5652]]]]) (torch.Size([1, 3, 3, 5]))\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).float()\n",
    "w_base= np.round(np.random.uniform(-1,1,size=(out_ch,in_ch)+(1,1)+kernel_shape),2)\n",
    "b_base= np.round(np.random.uniform(-1,1,size=(out_ch,)+(1,1)),2)\n",
    "w= np.repeat(np.repeat(w_base,out_shape[1],axis=3),out_shape[0],axis=2)\n",
    "b= np.repeat(np.repeat(b_base,out_shape[1],axis=2),out_shape[0],axis=1)\n",
    "# print(f'img_torch= {img}, {img.shape}')\n",
    "print(f'w= {w[0:1,0:1,0:1,0:1,:,:]}..., {w.shape}, {w_base.shape}')\n",
    "print(f'b= {b}..., {b.shape}, {b_base.shape}')\n",
    "w_base_torch= torch.from_numpy(w_base).reshape(w_base.shape[:2]+w_base.shape[4:]).float()\n",
    "b_base_torch= torch.from_numpy(b_base).reshape(-1).float()\n",
    "# b_base_torch= None\n",
    "# print(f'w_base_torch= {w_base_torch}..., {w_base_torch.shape}')\n",
    "# print(f'b_base_torch= {b_base_torch}..., {b_base_torch.shape}')\n",
    "assert(padding_mode in valid_padding_modes)\n",
    "# valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "# factory_kwargs= {'device': None, 'dtype': None}\n",
    "if padding_mode=='zeros':\n",
    "  res_gt= torch.nn.functional.conv2d(img_torch, w_base_torch, b_base_torch, stride_, padding_)\n",
    "else:\n",
    "  img_torch= torch.nn.functional.pad(img_torch, tuple(x for x in reversed(padding_) for _ in range(2)), mode=padding_mode)\n",
    "  res_gt= torch.nn.functional.conv2d(img_torch, w_base_torch, b_base_torch, stride_, torch.nn.modules.utils._pair(0))\n",
    "print(f'res_gt= {res_gt} ({res_gt.shape})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510d678-faee-4753-b368-e09e4a79b63e",
   "metadata": {},
   "source": [
    "### With Torch (wo torch.conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d2712246-ecbd-403d-884e-4b4d2c4697d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view_shape= torch.Size([1, 2, 3, 5, 3, 3])\n",
      "strides= (192, 96, 24, 2, 12, 1)\n",
      "sub_matrices.shape= torch.Size([1, 2, 3, 5, 3, 3])\n",
      "res_tch= tensor([[[[ 1.9333,  1.3860, -0.3644, -2.6978, -1.1301],\n",
      "          [ 0.5294, -1.7122, -0.4804, -2.8249, -1.0778],\n",
      "          [ 2.2135,  0.8713, -2.0962,  0.0470,  0.1462]],\n",
      "\n",
      "         [[ 0.8562,  0.4286,  0.1503,  0.8111,  1.5066],\n",
      "          [-0.7512,  1.2271,  2.1082, -2.6052,  1.4239],\n",
      "          [ 1.3573, -3.5836, -0.0464, -1.4093,  0.1911]],\n",
      "\n",
      "         [[ 0.0108, -0.4411, -1.1051,  4.2101,  1.4716],\n",
      "          [-0.4302,  0.0436,  0.8183,  1.9086,  0.6192],\n",
      "          [-0.9203, -0.4216,  0.5768,  1.5044,  0.8749]]]]) (torch.Size([1, 3, 3, 5])), N/A\n"
     ]
    }
   ],
   "source": [
    "img_torch= torch.from_numpy(img).float()\n",
    "w_torch= torch.from_numpy(w).float()\n",
    "b_torch= torch.from_numpy(b).float()\n",
    "view_shape= img_torch.shape[:2]+out_shape+kernel_shape\n",
    "print(f'view_shape= {view_shape}')\n",
    "if padding_mode=='zeros':\n",
    "  img_torch= torch.nn.functional.pad(img_torch, tuple(x for x in reversed(padding_) for _ in range(2)), mode='constant', value=0.0)\n",
    "else:\n",
    "  img_torch= torch.nn.functional.pad(img_torch, tuple(x for x in reversed(padding_) for _ in range(2)), mode=padding_mode)\n",
    "strides= img_torch.stride()[:2]+tuple(np.array(img_torch.stride()[2:])*stride_)+img_torch.stride()[2:]\n",
    "print(f'strides= {strides}')\n",
    "sub_matrices= torch.as_strided(img_torch, view_shape, strides)\n",
    "print(f'sub_matrices.shape= {sub_matrices.shape}')\n",
    "# print(f'sub_matrices= {sub_matrices}')\n",
    "res_tch= torch.einsum('ijmnkl,bjmnkl->bimn', w_torch, sub_matrices)+b_torch\n",
    "print(f'res_tch= {res_tch} ({res_tch.shape}), {torch.all(torch.isclose(res_tch,res_gt)) if res_gt is not None else \"N/A\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ddb25-0bdc-41b2-bef1-03dcfe09b234",
   "metadata": {},
   "source": [
    "### Modular version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "171495e3-3f2d-4847-9fa4-153214024462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_lc= tensor([[[[ 1.9333,  1.3860, -0.3644, -2.6978, -1.1301],\n",
      "          [ 0.5294, -1.7122, -0.4804, -2.8249, -1.0778],\n",
      "          [ 2.2135,  0.8713, -2.0962,  0.0470,  0.1462]],\n",
      "\n",
      "         [[ 0.8562,  0.4286,  0.1503,  0.8111,  1.5066],\n",
      "          [-0.7512,  1.2271,  2.1082, -2.6052,  1.4239],\n",
      "          [ 1.3573, -3.5836, -0.0464, -1.4093,  0.1911]],\n",
      "\n",
      "         [[ 0.0108, -0.4411, -1.1051,  4.2101,  1.4716],\n",
      "          [-0.4302,  0.0436,  0.8183,  1.9086,  0.6192],\n",
      "          [-0.9203, -0.4216,  0.5768,  1.5044,  0.8749]]]],\n",
      "       grad_fn=<AddBackward0>) (torch.Size([1, 3, 3, 5])), True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "in_shape: Tuple or list of (in_channels, in_h, in_w).\n",
    "'''\n",
    "class TLocallyConnected2d(torch.nn.Module):\n",
    "  def __init__(self, in_shape, out_channels, kernel_size, stride=1, padding=0, bias=True, padding_mode='zeros', device=None, dtype=None):\n",
    "    super(TLocallyConnected2d, self).__init__()\n",
    "    self.kernel_size= torch.nn.modules.utils._pair(kernel_size)\n",
    "    self.stride= torch.nn.modules.utils._pair(stride)\n",
    "    self.padding= torch.nn.modules.utils._pair(padding)\n",
    "    out_h= int((in_shape[1]+2*self.padding[0]-1*(self.kernel_size[0]-1)-1)/self.stride[0]+1)\n",
    "    out_w= int((in_shape[2]+2*self.padding[1]-1*(self.kernel_size[1]-1)-1)/self.stride[1]+1)\n",
    "    self.out_shape= (out_h,out_w)\n",
    "    valid_padding_modes= {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "    assert(padding_mode in valid_padding_modes)\n",
    "    self.padding_mode= padding_mode\n",
    "    factory_kwargs= dict(device=device, dtype=dtype)\n",
    "    self.weight= torch.nn.Parameter(torch.empty((out_channels,in_shape[0])+self.out_shape+self.kernel_size, **factory_kwargs))\n",
    "    if bias:\n",
    "      self.bias= torch.nn.Parameter(torch.empty((out_channels,)+self.out_shape, **factory_kwargs))\n",
    "    else:\n",
    "      self.register_parameter('bias', None)\n",
    "\n",
    "  def forward(self, x):\n",
    "    view_shape= x.shape[:2]+self.out_shape+self.kernel_size\n",
    "    if self.padding_mode=='zeros':\n",
    "      x= torch.nn.functional.pad(x, tuple(p for p in reversed(self.padding) for _ in range(2)), mode='constant', value=0.0)\n",
    "    else:\n",
    "      x= torch.nn.functional.pad(x, tuple(p for p in reversed(self.padding) for _ in range(2)), mode=padding_mode)\n",
    "    strides= x.stride()[:2]+tuple(np.array(x.stride()[2:])*self.stride)+x.stride()[2:]\n",
    "    sub_matrices= torch.as_strided(x, view_shape, strides)\n",
    "    x= torch.einsum('ijmnkl,bjmnkl->bimn', self.weight, sub_matrices)\n",
    "    return x if self.bias is None else x+self.bias\n",
    "\n",
    "lc= TLocallyConnected2d(img.shape[1:], out_ch, ks, stride=stride, padding=padding, bias=True, padding_mode=padding_mode)\n",
    "lc.weight.data= torch.from_numpy(w).float()\n",
    "lc.bias.data= torch.from_numpy(b).float()\n",
    "img_torch= torch.from_numpy(img).float()\n",
    "res_lc= lc(img_torch)\n",
    "print(f'res_lc= {res_lc} ({res_lc.shape}), {torch.all(torch.isclose(res_tch,res_lc))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "71459a34-acf3-46ec-b2d0-90f9f8fbecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred= tensor([[0.2714]], grad_fn=<AddmmBackward>)\n",
      "loss= 0.07367976754903793\n",
      "fc.weight.shape= torch.Size([1, 45])\n",
      "fc.weight.grad.shape= torch.Size([1, 45])\n",
      "fc.bias.shape= torch.Size([1])\n",
      "fc.bias.grad.shape= torch.Size([1])\n",
      "lc.weight.shape= torch.Size([3, 2, 3, 5, 3, 3])\n",
      "lc.weight.grad.shape= torch.Size([3, 2, 3, 5, 3, 3])\n",
      "lc.bias.shape= torch.Size([3, 3, 5])\n",
      "lc.bias.grad.shape= torch.Size([3, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "fc= torch.nn.Linear(torch.flatten(lc(img_torch)).shape[0], 1)\n",
    "net= torch.nn.Sequential(lc, torch.nn.Flatten(), fc)\n",
    "# net(img_torch)\n",
    "torchinfo.summary(net)\n",
    "opt= torch.optim.Adam(net.parameters())\n",
    "f_loss= torch.nn.MSELoss()\n",
    "opt.zero_grad()\n",
    "pred= net(img_torch)\n",
    "loss= f_loss(pred, torch.tensor([[0.0]]))\n",
    "loss.backward()\n",
    "print(f'pred= {pred}')\n",
    "print(f'loss= {loss}')\n",
    "print(f'fc.weight.shape= {fc.weight.shape}')\n",
    "print(f'fc.weight.grad.shape= {fc.weight.grad.shape}')\n",
    "print(f'fc.bias.shape= {fc.bias.shape}')\n",
    "print(f'fc.bias.grad.shape= {fc.bias.grad.shape}')\n",
    "print(f'lc.weight.shape= {lc.weight.shape}')\n",
    "print(f'lc.weight.grad.shape= {lc.weight.grad.shape}')\n",
    "print(f'lc.bias.shape= {lc.bias.shape}')\n",
    "print(f'lc.bias.grad.shape= {lc.bias.grad.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
